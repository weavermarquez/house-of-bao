#+title: Laws of Form Arithmetic Evaluation Suite
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python ".venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name    | Rule           | Interpretation      |
|-------+---------+----------------+---------------------|
| I1    | Number  | =()()= → =()=  | Condense/Confirm    |
| I2    | Order   | =(())= → void  | Cancel/Compensate   |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (the mark) — equivalent to TRUE or 1
- void (empty) — equivalent to FALSE or 0

* Install Packages
#+begin_src bash
uv sync
#+end_src

#+RESULTS:

* Setup

#+begin_src python
import os
import json
import random
import re
from datetime import datetime
from typing import Tuple, List, Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# API clients (we'll initialize these when needed)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# Model defaults
MODELS = {
    "openai": "gpt-5.1",
    "anthropic": "claude-opus-4-5-20251101",
    "google": "gemini-3-pro-preview"
}

print("Setup complete.")
print(f"OpenAI key: {'set' if OPENAI_API_KEY else 'missing'}")
print(f"Anthropic key: {'set' if ANTHROPIC_API_KEY else 'missing'}")
print(f"Gemini key: {'set' if GEMINI_API_KEY else 'missing'}")
#+end_src

#+RESULTS:
: Setup complete.
: OpenAI key: set
: Anthropic key: set
: Gemini key: set

* Form Representation

We use two representations:
- *Internal*: Nested Python lists. =[[]]= represents =(())=, =[[], []]= represents =()()=
- *String*: Parentheses notation for display and LLM prompts

#+begin_src python
def form_to_string(form: list) -> str:
    """Convert internal form representation to string."""
    if not form:
        return ""
    return "".join(f"({form_to_string(child)})" for child in form)

def string_to_form(s: str) -> list:
    """Parse string notation to internal form."""
    result = []
    i = 0
    while i < len(s):
        if s[i] == '(':
            # Find matching close paren
            depth = 1
            j = i + 1
            while j < len(s) and depth > 0:
                if s[j] == '(':
                    depth += 1
                elif s[j] == ')':
                    depth -= 1
                j += 1
            # Recursively parse contents
            result.append(string_to_form(s[i+1:j-1]))
            i = j
        else:
            i += 1
    return result

# Test
test_forms = ["()", "(())", "()()", "((()))", "(()())"]
for s in test_forms:
    f = string_to_form(s)
    back = form_to_string(f)
    print(f"{s:12} -> {str(f):20} -> {back}")
#+end_src

#+RESULTS:
: ()           -> [[]]                 -> ()
: (())         -> [[[]]]               -> (())
: ()()         -> [[], []]             -> ()()
: ((()))       -> [[[[]]]]             -> ((()))
: (()())       -> [[[], []]]           -> (()())

* Form Generator

Generate random well-formed LoF expressions with controlled complexity.

#+begin_src python
def form_depth(form: list) -> int:
    """Calculate the nesting depth of a form (list representation)."""
    if not form:
        return 0
    return 1 + max(form_depth(child) for child in form)


def string_depth(s: str) -> int:
    """Calculate the nesting depth of a form string. O(n) and no allocation."""
    max_depth = 0
    current = 0
    for c in s:
        if c == '(':
            current += 1
            max_depth = max(max_depth, current)
        elif c == ')':
            current -= 1
    return max_depth


def generate_form_string(min_depth: int = 1, max_depth: int = 3, max_width: int = 3, max_marks: int = 200) -> str:
    """
    Generate a random LoF form as a string directly (memory efficient).

    Args:
        min_depth: Minimum nesting depth (guaranteed)
        max_depth: Maximum nesting depth
        max_width: Maximum number of adjacent forms at any level
        max_marks: Maximum total number of marks (parenthesis pairs) to prevent explosion

    Returns:
        A form as a string, e.g. "(()())" or ""
    """
    marks_used = [0]  # Mutable counter

    def build(remaining_min: int, remaining_max: int) -> str:
        if remaining_max <= 0 or marks_used[0] >= max_marks:
            if marks_used[0] < max_marks and random.random() > 0.5:
                marks_used[0] += 1
                return "()"
            return ""

        if remaining_min > 0:
            width = random.randint(1, max_width)
        else:
            width = random.randint(0, max_width)
            if width == 0:
                return ""

        # One child guaranteed deep, others can be shallow
        parts = []
        deep_idx = random.randint(0, width - 1) if remaining_min > 0 else -1
        for i in range(width):
            if marks_used[0] >= max_marks:
                break
            marks_used[0] += 1
            if i == deep_idx:
                inner = build(remaining_min - 1, remaining_max - 1)
            else:
                inner = build(0, remaining_max - 1)
            parts.append(f"({inner})")
        return "".join(parts)

    return build(min_depth, max_depth)


# Generate some examples
random.seed(1010101423)
print("Sample generated forms (min_depth=2, max_depth=3):")
for i in range(10):
    s = generate_form_string(min_depth=2, max_depth=3, max_width=2)
    d = string_depth(s)
    print(f"  {i+1:2}. depth={d}: {s if s else '<void>'}")
#+end_src

#+RESULTS:
#+begin_example
Sample generated forms (min_depth=2, max_depth=3):
   1. depth=4: ((())((())))
   2. depth=4: (((()))(()(())))
   3. depth=4: ((())((())))
   4. depth=4: (((())))((()))
   5. depth=2: (()())
   6. depth=4: (((())()))
   7. depth=4: (((())))
   8. depth=3: ((()))((()())(()))
   9. depth=4: (((())())(()))
  10. depth=4: ((()(()))((())()))(((())(())))
#+end_example

* Ground-Truth Simplifier

O(n) stack-based simplifier. Single pass, applies I1 and I2 as marks close.

#+begin_src python
def simplify_string(s: str) -> Tuple[str, List[Tuple[str, str, str]]]:
    """
    O(n) simplification with step tracking.

    Returns:
        (canonical_form, steps) where steps is [(before, after, axiom), ...]
    """
    stack: List[List[str]] = [[]]  # Stack of frames, each frame is list of child forms
    steps = []

    for c in s:
        if c == '(':
            stack.append([])
        elif c == ')':
            children = stack.pop()

            # Apply I1: all identical → condense
            if len(children) > 1 and all(ch == children[0] for ch in children):
                before = "(" + "".join(children) + ")"
                children = [children[0]]
                after = "(" + "".join(children) + ")"
                steps.append((before, after, "I1"))

            # Apply I2: single mark inside → cancel to void
            content = "".join(children)
            if content == "()":
                before = "(())"
                steps.append((before, "", "I2"))
                # Don't append anything to parent (void)
            else:
                # Append this mark to parent
                stack[-1].append("(" + content + ")")

    # Root level
    result = stack[0]

    # Apply I1 at root if needed
    if len(result) > 1 and all(ch == result[0] for ch in result):
        before = "".join(result)
        result = [result[0]]
        after = "".join(result)
        steps.append((before, after, "I1"))

    final = "".join(result)
    return final if final else "void", steps


def canonical_string(s: str) -> str:
    """Get the canonical (simplified) form of a string."""
    result, _ = simplify_string(s)
    return result


# Test the simplifier
print("Simplification examples:")
test_exprs = ["()()", "(())", "((()))", "(()())", "()(())", "(()(()))", "(())()", "(())(())"]
for expr in test_exprs:
    result, steps = simplify_string(expr)
    print(f"\n{expr} -> {result}")
    for before, after, axiom in steps:
        after_display = after if after else "void"
        print(f"  {before} -> {after_display} [{axiom}]")
#+end_src

#+RESULTS:
#+begin_example
Simplification examples:

()() -> ()
  ()() -> () [I1]

(()) -> void
  (()) -> void [I2]

((())) -> ()
  (()) -> void [I2]

(()()) -> void
  (()()) -> (()) [I1]
  (()) -> void [I2]

()(()) -> ()
  (()) -> void [I2]

(()(())) -> void
  (()) -> void [I2]
  (()) -> void [I2]

(())() -> ()
  (()) -> void [I2]

(())(()) -> void
  (()) -> void [I2]
  (()) -> void [I2]
#+end_example

* Test Case Generation

Generate a reproducible test suite across difficulty levels.

Difficulty is defined by *minimum* depth:
- Easy: depth 1-2 (simple marks and single nesting)
- Medium: depth 2-3 (requires at least one I2 application)
- Hard: depth 3-4 (multiple nested reductions)

#+begin_src python
def generate_test_cases(n: int = 500, seed: int = 2024) -> List[dict]:
    """Generate n test cases with varying difficulty."""
    random.seed(seed)
    cases = []

    # Distribution with (difficulty, min_depth, max_depth, max_width, max_marks)
    # max_marks caps complexity to keep simplification fast
    difficulties = (
        [("1. easy", 1, 2, 4, 20)] * 100 +
        [("2. medium", 3, 4, 4, 30)] * 100 +
        [("3. hard", 5, 6, 5, 50)] * 100 +
        [("4. lunatic", 7, 8, 5, 70)] * 100 +
        [("5. extra", 9, 10, 6, 100)] * 100
    )
    random.shuffle(difficulties)

    for i, (diff, min_d, max_d, max_w, max_m) in enumerate(difficulties[:n]):
        input_str = generate_form_string(min_depth=min_d, max_depth=max_d, max_width=max_w, max_marks=max_m)
        if not input_str:
            input_str = "()"  # Ensure non-empty
        depth = string_depth(input_str)

        # O(n) simplification directly on string
        target, steps = simplify_string(input_str)

        cases.append({
            "id": f"lof_{i+1:03d}",
            "input": input_str,
            "target": target,
            "difficulty": diff,
            "depth": depth,
            "steps": len(steps)
        })

    return cases


# Generate and preview test cases
test_cases = generate_test_cases(500)
print(f"Generated {len(test_cases)} test cases\n")

# Preview distribution
from collections import Counter
diff_counts = Counter(c["difficulty"] for c in test_cases)
target_counts = Counter(c["target"] for c in test_cases)
depth_by_diff = {}
for c in test_cases:
    depth_by_diff.setdefault(c["difficulty"], []).append(c["depth"])

print("By difficulty:")
for diff in ["1. easy", "2. medium", "3. hard", "4. lunatic", "5. extra"]:
    depths = depth_by_diff.get(diff, [])
    avg_depth = sum(depths) / len(depths) if depths else 0
    print(f"  {diff}: {diff_counts[diff]} cases, avg depth={avg_depth:.1f}, range=[{min(depths)}-{max(depths)}]")

print("\nBy target value:")
for target, count in sorted(target_counts.items()):
    print(f"  {target}: {count}")

print("\nSample cases:")
for case in test_cases[:20]:
    print(f"  {case['id']}: {case['input']:20} -> {case['target']:6} (d={case['depth']}, {case['difficulty']}, {case['steps']} steps)")
#+end_src

#+RESULTS:
#+begin_example
Generated 500 test cases

By difficulty:
  1. easy: 100 cases, avg depth=2.8, range=[1-3]
  2. medium: 100 cases, avg depth=5.0, range=[3-5]
  3. hard: 100 cases, avg depth=7.0, range=[7-7]
  4. lunatic: 100 cases, avg depth=9.0, range=[9-9]
  5. extra: 100 cases, avg depth=11.0, range=[11-11]

By target value:
  (): 258
  void: 242

Sample cases:
  lof_001: (((()(()(()(())(())(())(()))((())()(())()())()(()(())()()(())))((()())((())()()()))()(((())())())))) -> void   (d=7, 3. hard, 26 steps)
  lof_002: (()(())(((((()(((()()()(())(())))()(((())()(())(())(())())((())()(())(())(()))))(((()(())(()))((())()(())(())()())((())()(())(())()(())))(((())(())(())(()))((())(()))((())()()))(((())(())(())))))))))) -> void   (d=11, 5. extra, 52 steps)
  lof_003: ((()))(((((()(())(()(()()()(()))((())(())()(())))()(((()))(()())(())()(())))())(()(()(()(())()((()))((())))((())))(((()(())(())(()))())))))) -> ()     (d=9, 4. lunatic, 37 steps)
  lof_004: ((((())())))()((((()))((())()())((())(())(())(())))((()()))) -> ()     (d=5, 2. medium, 16 steps)
  lof_005: ((()((())()(())(())))((()(())(()))(()()()(()))(()()(()))))() -> ()     (d=5, 2. medium, 15 steps)
  lof_006: ((((((())()(())(())())((())()(())(()))((())(()))((())))))(((((())()(())())((())())())((()))((()))))) -> void   (d=7, 3. hard, 27 steps)
  lof_007: ((((((((())(()))((())()()()()))()(((())()()(())(()))()(()()()(()))())()))((((()()()(()))(()(()))()()((())(())()()(()))))(((())((())()))))))) -> void   (d=9, 4. lunatic, 37 steps)
  lof_008: ((((((()))(())((())()(())()(()))))((((())()(())()())(()()()(())(()))((())(())()()())((())(())()))))) -> ()     (d=7, 3. hard, 25 steps)
  lof_009: ()(()((((((()()(()))((())()(())()(()))((())(())(())(())))((())(()()()()())((())(())(())(())())))(((()(()))(()()(()))()(()(()))())((()))))))) -> ()     (d=9, 4. lunatic, 36 steps)
  lof_010: ((((()(()()()()((()(())()()())((())(())(())())()))((((())()()(())())(()(())(()))((())()(())(()))((())()()(()))))())((((())(()(())(())))))))) -> void   (d=9, 4. lunatic, 35 steps)
  lof_011: (((((()(((())(()))(()(())(())())((())()))((()()))(((())(())(())()(()))(()(())()))(()()()((())()())(()()(()))))((()(()(())(())(())(())))))))) -> ()     (d=9, 4. lunatic, 37 steps)
  lof_012: ((((())(((()(((()()(())())((())(())(())(())())(()(())(()))(()(())())(()()()))((()())()()())))()((((()(())(())()(())(()))(()(())()()(()))(()(())()(())(())(()))(()(())())((())()()))(()((())()()))))))))) -> void   (d=11, 5. extra, 54 steps)
  lof_013: ((((()(()(())()()()))((()(())(())()(()))((())(())(())()()))(((())(())())((())(())()(())())))(()()))) -> void   (d=7, 3. hard, 27 steps)
  lof_014: ((())()())()((())(())()) -> ()     (d=3, 1. easy, 6 steps)
  lof_015: ((()((())(())))(((())())((())())(()()(())(()))())(((()))())) -> ()     (d=5, 2. medium, 16 steps)
  lof_016: ((((((())(())(())()(()))))((((())()(())()())(()())((())()()(()))((()))())))((((()()))(((())())())))) -> void   (d=7, 3. hard, 27 steps)
  lof_017: ((((())(())())))(((()(()))))(()((()(()))((())()(())(())))()) -> void   (d=5, 2. medium, 15 steps)
  lof_018: ((((()((((())()()(())(()))))((()()((())())((())()(())()())())())(()((()(())()(())(()))((()))((())())(())())(((())(()))((())(())()))())())))) -> ()     (d=9, 4. lunatic, 37 steps)
  lof_019: ((()))((((())(()))(()()()())((())(())()))(()(()(())(())()))) -> ()     (d=5, 2. medium, 15 steps)
  lof_020: ((((((((())(())())((())())()((())))((()()(())()())((())()(()))((())))(((())(())()(())))(()((())(())(()))(()(())(())()())((())()()()()))))))) -> void   (d=9, 4. lunatic, 35 steps)
#+end_example


* Save Test Cases

#+begin_src python
# Save test cases for reproducibility
with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
print(f"Saved {len(test_cases)} test cases to test_cases.json")
#+end_src

#+RESULTS:
: Saved 500 test cases to test_cases.json

* Load Test Cases

Skip test generation by loading previously saved cases.

#+begin_src python
# Load saved test cases (skip generation)
with open("test_cases.json", "r") as f:
    test_cases = json.load(f)
print(f"Loaded {len(test_cases)} test cases from test_cases.json")
#+end_src

#+RESULTS:
: Loaded 30 test cases from test_cases.json

* Set up Prompt

Initialize API clients for each provider.

#+begin_src python
PROMPT_TEMPLATE = """Simplify this Laws of Form expression using the two axioms:

I1 (Number): ()() = ()
   Multiple adjacent marks condense to a single mark.

I2 (Order): (()) = void
   A mark containing only a mark cancels to void (nothing).

Expression: {expression}

Apply axioms until you reach () or void.

Structure your response EXACTLY as follows:

<scratchpad>
Think through the problem here. Work out the simplification in whatever way helps you.
</scratchpad>

<trace>
STEP 1: full_expression => simplified_expression [I1 or I2]
STEP 2: ...
...continue until done...
</trace>

<answer>() or void</answer>

Rules for the trace:
- Each STEP must show the COMPLETE expression before and after
- Use exactly [I1] or [I2] to indicate which axiom
- No explanations inside <trace>, only STEP lines
"""


def extract_answer(response: str) -> str:
    """Extract the final answer from LLM response."""
    # Look for <answer> tag first
    match = re.search(r'<answer>\s*(\(\)|void)\s*</answer>', response, re.IGNORECASE)
    if match:
        ans = match.group(1).lower()
        return "()" if ans == "()" else "void"

    # Fallback: FINAL: pattern
    match = re.search(r'FINAL:\s*(\(\)|void)', response, re.IGNORECASE)
    if match:
        ans = match.group(1).lower()
        return "()" if ans == "()" else "void"

    # Last resort: look for last occurrence of () or void
    response_lower = response.lower()
    last_mark = response_lower.rfind("()")
    last_void = max(response_lower.rfind("void"), response_lower.rfind("empty"))

    if last_mark > last_void:
        return "()"
    elif last_void > last_mark:
        return "void"

    return "unknown"


# === Trace Validation ===

def extract_trace_block(response: str) -> str:
    """Extract content from <trace>...</trace> block, or return full response."""
    match = re.search(r'<trace>(.*?)</trace>', response, re.DOTALL | re.IGNORECASE)
    if match:
        return match.group(1)
    return response  # Fallback to full response


def parse_trace_steps(response: str) -> List[Tuple[str, str, str]]:
    """
    Parse STEP lines from LLM response.

    Returns:
        List of (before, after, axiom) tuples
    """
    # Extract trace block if present
    trace_text = extract_trace_block(response)

    # Pattern: STEP k: expr => expr' [I1] or [I2]
    # Handle variations in whitespace and formatting
    pattern = r'STEP\s*\d+\s*:\s*([^\s].*?)\s*=>\s*([^\s].*?)\s*\[(I[12])\]'
    matches = re.findall(pattern, trace_text, re.IGNORECASE)

    steps = []
    for before, after, axiom in matches:
        # Clean up expressions (remove backticks, extra whitespace)
        before = before.strip().strip('`').strip()
        after = after.strip().strip('`').strip()
        # Normalize void
        if after.lower() == 'void' or after == '':
            after = ''
        axiom = axiom.upper()
        steps.append((before, after, axiom))

    return steps


def find_i1_sites(expr: str) -> List[Tuple[int, int, str]]:
    """
    Find all positions where I1 (condensation) can apply.

    Returns:
        List of (start, end, condensed) tuples where expr[start:end] can condense
    """
    sites = []
    # Use stack-based parsing to find adjacent identical marks
    stack = [[]]  # Each frame: list of (start_pos, content) tuples

    start_positions = []

    for i, c in enumerate(expr):
        if c == '(':
            start_positions.append(i)
            stack.append([])
        elif c == ')':
            # Guard against malformed expressions
            if not start_positions or len(stack) < 2:
                return []  # Malformed - return empty
            start = start_positions.pop()
            children = stack.pop()
            content = expr[start:i+1]
            stack[-1].append((start, content))

            # Check if this level has adjacent identical marks
            if len(stack[-1]) > 1:
                contents = [c for _, c in stack[-1]]
                if all(c == contents[0] for c in contents):
                    # All adjacent marks are identical - I1 applies
                    first_start = stack[-1][0][0]
                    last_end = i + 1
                    sites.append((first_start, last_end, contents[0]))

    # Check root level (only if balanced)
    if start_positions:  # Unbalanced - unclosed parens
        return []
    if len(stack[0]) > 1:
        contents = [c for _, c in stack[0]]
        if all(c == contents[0] for c in contents):
            sites.append((0, len(expr), contents[0]))

    return sites


def find_i2_sites(expr: str) -> List[Tuple[int, int]]:
    """
    Find all positions where I2 (cancellation) can apply.

    Returns:
        List of (start, end) tuples where expr[start:end] == "(())"
    """
    sites = []
    i = 0
    while i < len(expr):
        if expr[i:i+4] == '(())':
            sites.append((i, i+4))
            i += 4
        else:
            i += 1
    return sites


def validate_step(before: str, after: str, axiom: str) -> Tuple[bool, str]:
    """
    Validate a single reasoning step.

    Returns:
        (is_valid, error_type) where error_type is one of:
        - "" (no error)
        - "wrong_axiom" (axiom doesn't apply)
        - "scope_collapse" (correct inner transform but lost outer context)
        - "wrong_result" (axiom applies but result is wrong)
        - "malformed" (can't parse expressions)
    """
    # Check for malformed expressions
    def balanced(s):
        depth = 0
        for c in s:
            if c == '(':
                depth += 1
            elif c == ')':
                depth -= 1
            if depth < 0:
                return False
        return depth == 0

    if not balanced(before) or (after and not balanced(after)):
        return False, "malformed"

    if axiom == "I1":
        sites = find_i1_sites(before)
        if not sites:
            return False, "wrong_axiom"

        # Check if any valid I1 application produces 'after'
        for start, end, condensed in sites:
            expected = before[:start] + condensed + before[end:]
            if expected == after:
                return True, ""

            # Check for scope collapse: inner condensation correct but outer lost
            # This happens when before = (X) and they output X' instead of (X')
            if before.startswith('(') and before.endswith(')'):
                inner_before = before[1:-1]
                inner_sites = find_i1_sites(inner_before)
                for istart, iend, icondensed in inner_sites:
                    inner_expected = inner_before[:istart] + icondensed + inner_before[iend:]
                    if after == inner_expected:
                        return False, "scope_collapse"

        return False, "wrong_result"

    elif axiom == "I2":
        sites = find_i2_sites(before)
        if not sites:
            return False, "wrong_axiom"

        # Check if any valid I2 application produces 'after'
        for start, end in sites:
            expected = before[:start] + before[end:]
            if expected == after:
                return True, ""

            # Check for scope collapse
            if before.startswith('(') and before.endswith(')'):
                inner_before = before[1:-1]
                inner_sites = find_i2_sites(inner_before)
                for istart, iend in inner_sites:
                    inner_expected = inner_before[:istart] + inner_before[iend:]
                    if after == inner_expected:
                        return False, "scope_collapse"

        return False, "wrong_result"

    return False, "wrong_axiom"


def validate_trace(response: str, input_expr: str) -> dict:
    """
    Validate all reasoning steps in an LLM response.

    Returns:
        {
            "steps_parsed": int,
            "steps_valid": int,
            "errors": [(step_num, error_type, before, after, axiom), ...],
            "trace_score": float  # steps_valid / steps_parsed
        }
    """
    steps = parse_trace_steps(response)

    if not steps:
        return {
            "steps_parsed": 0,
            "steps_valid": 0,
            "errors": [],
            "trace_score": 0.0
        }

    valid_count = 0
    errors = []

    for i, (before, after, axiom) in enumerate(steps):
        is_valid, error_type = validate_step(before, after, axiom)
        if is_valid:
            valid_count += 1
        else:
            errors.append((i + 1, error_type, before, after, axiom))

    return {
        "steps_parsed": len(steps),
        "steps_valid": valid_count,
        "errors": errors,
        "trace_score": valid_count / len(steps) if steps else 0.0
    }


# Test trace validation
print("=== Trace Validation Tests ===\n")

# Test I2 validation
print("I2 (Order) tests:")
valid, err = validate_step("(())", "", "I2")
print(f"  (()) -> void [I2]: valid={valid}, err='{err}'")

valid, err = validate_step("()(())", "()", "I2")
print(f"  ()(()) -> () [I2]: valid={valid}, err='{err}'")

valid, err = validate_step("((()))", "()", "I2")
print(f"  ((())) -> () [I2]: valid={valid}, err='{err}'")  # Should fail - wrong result

valid, err = validate_step("((()))", "", "I2")
print(f"  ((())) -> void [I2]: valid={valid}, err='{err}'")  # Scope collapse!

# Test I1 validation
print("\nI1 (Number) tests:")
valid, err = validate_step("()()", "()", "I1")
print(f"  ()() -> () [I1]: valid={valid}, err='{err}'")

valid, err = validate_step("(()())", "(())", "I1")
print(f"  (()()) -> (()) [I1]: valid={valid}, err='{err}'")

valid, err = validate_step("(()())", "()", "I1")
print(f"  (()()) -> () [I1]: valid={valid}, err='{err}'")  # Scope collapse!

# Test parsing from realistic response (old format - fallback)
print("\nParsing test (old format):")
sample_response_old = """
STEP 1: (())(()) => (()) [I1]
STEP 2: (()) => void [I2]
FINAL: void
"""
steps = parse_trace_steps(sample_response_old)
print(f"  Parsed {len(steps)} steps: {steps}")

# Test new structured format
print("\nParsing test (new structured format):")
sample_response_new = """
<scratchpad>
Let me work through this. I see (())(()) which has two (()) marks.
They're identical so I1 applies. Then (()) cancels by I2.
</scratchpad>

<trace>
STEP 1: (())(()) => (()) [I1]
STEP 2: (()) => void [I2]
</trace>

<answer>void</answer>
"""
steps = parse_trace_steps(sample_response_new)
print(f"  Parsed {len(steps)} steps: {steps}")
answer = extract_answer(sample_response_new)
print(f"  Extracted answer: {answer}")

validation = validate_trace(sample_response_new, "(())(())")
print(f"  Validation: {validation['steps_valid']}/{validation['steps_parsed']} valid")
print(f"  Trace score: {validation['trace_score']:.1%}")
#+end_src

#+RESULTS:
#+begin_example
=== Trace Validation Tests ===

I2 (Order) tests:
  (()) -> void [I2]: valid=True, err=''
  ()(()) -> () [I2]: valid=True, err=''
  ((())) -> () [I2]: valid=True, err=''
  ((())) -> void [I2]: valid=False, err='scope_collapse'

I1 (Number) tests:
  ()() -> () [I1]: valid=True, err=''
  (()()) -> (()) [I1]: valid=True, err=''
  (()()) -> () [I1]: valid=False, err='scope_collapse'

Parsing test (old format):
  Parsed 2 steps: [('(())(())', '(())', 'I1'), ('(())', '', 'I2')]

Parsing test (new structured format):
  Parsed 2 steps: [('(())(())', '(())', 'I1'), ('(())', '', 'I2')]
  Extracted answer: void
  Validation: 2/2 valid
  Trace score: 100.0%
#+end_example

* Async Eval Functions

Async evaluation using context managers for clean client lifecycle.

#+begin_src python
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from google import genai
import asyncio


async def eval_openai_async(expression: str, model: str = MODELS["openai"]) -> Tuple[str, str, str]:
    """Evaluate using OpenAI API (async). Returns (response, answer, model)."""
    if not OPENAI_API_KEY:
        return "", "no_client", model
    prompt = PROMPT_TEMPLATE.format(expression=expression)
    async with AsyncOpenAI() as client:
        response = await client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
    text = response.choices[0].message.content
    return text, extract_answer(text), model


async def eval_anthropic_async(expression: str, model: str = MODELS["anthropic"]) -> Tuple[str, str, str]:
    """Evaluate using Anthropic API (async). Returns (response, answer, model)."""
    if not ANTHROPIC_API_KEY:
        return "", "no_client", model
    prompt = PROMPT_TEMPLATE.format(expression=expression)
    async with AsyncAnthropic() as client:
        response = await client.messages.create(
            model=model,
            max_tokens=1024,
            messages=[{"role": "user", "content": prompt}]
        )
    text = response.content[0].text
    return text, extract_answer(text), model


async def eval_google_async(expression: str, model: str = MODELS["google"]) -> Tuple[str, str, str]:
    """Evaluate using Google Gemini API (async). Returns (response, answer, model)."""
    if not GEMINI_API_KEY:
        return "", "no_client", model
    prompt = PROMPT_TEMPLATE.format(expression=expression)
    async with genai.Client().aio as client:
        response = await client.models.generate_content(model=model, contents=prompt)
    text = response.text
    return text, extract_answer(text), model


print("Async eval functions defined.")
#+end_src

#+RESULTS:
: Async eval functions defined.

* Sample Run (Async)

Run a small subset with async for quick iteration. Saves full outputs.

#+begin_src python
PROVIDERS = [
    ("openai", OPENAI_API_KEY, eval_openai_async),
    ("anthropic", ANTHROPIC_API_KEY, eval_anthropic_async),
    ("google", GEMINI_API_KEY, eval_google_async),
]


async def run_sample_async(cases: List[dict], n: int = 5) -> List[dict]:
    """Run async evaluation on sample cases."""
    sample = cases[:n]
    results = []

    # Build list of all (case, provider, eval_fn) combinations
    tasks = []
    task_info = []  # Track which case/provider each task belongs to

    for case in sample:
        for name, key, fn in PROVIDERS:
            if key:
                tasks.append(fn(case["input"]))
                task_info.append((case, name))

    # Run all in parallel
    responses = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results
    for (case, provider), resp in zip(task_info, responses):
        if isinstance(resp, Exception):
            response, answer, model = f"ERROR: {resp}", "error", "unknown"
        else:
            response, answer, model = resp

        results.append({
            "id": case["id"],
            "input": case["input"],
            "target": case["target"],
            "difficulty": case["difficulty"],
            "depth": case.get("depth", 0),
            "provider": provider,
            "model": model,
            "response": response,
            "extracted_answer": answer,
            "correct": answer == case["target"]
        })

    return results


# Run sample evaluation
print("Running async sample evaluation...")
sample_results = asyncio.run(run_sample_async(test_cases, n=5))

# Save full results
with open("sample_results.json", "w") as f:
    json.dump(sample_results, f, indent=2)
print(f"Saved {len(sample_results)} results to sample_results.json")

# Display summary
print("\n=== Sample Results ===")
for r in sample_results:
    status = "✅" if r["correct"] else "❌"
    print(f"{status} {r['id']} ({r['provider']}): {r['input'][:30]:30} -> {r['extracted_answer']} (target: {r['target']})")

# Show failures with reasoning
failures = [r for r in sample_results if not r["correct"]]
if failures:
    print(f"\n=== {len(failures)} Failures - Full Reasoning ===")
    for f in failures[:3]:
        print(f"\n--- {f['id']} ({f['model']}) ---")
        print(f"Input: {f['input']}")
        print(f"Expected: {f['target']}, Got: {f['extracted_answer']}")
        print(f"Response:\n{f['response'][:500]}...")
#+end_src

#+RESULTS:
#+begin_example
Running async sample evaluation...
Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
Saved 15 results to sample_results.json

=== Sample Results ===
✅ lof_001 (openai): (((()(()(()(())(())(())(()))(( -> void (target: void)
❌ lof_001 (anthropic): (((()(()(()(())(())(())(()))(( -> () (target: void)
❌ lof_001 (google): (((()(()(()(())(())(())(()))(( -> () (target: void)
✅ lof_002 (openai): (()(())(((((()(((()()()(())(() -> void (target: void)
❌ lof_002 (anthropic): (()(())(((((()(((()()()(())(() -> () (target: void)
✅ lof_002 (google): (()(())(((((()(((()()()(())(() -> void (target: void)
✅ lof_003 (openai): ((()))(((((()(())(()(()()()(() -> () (target: ())
✅ lof_003 (anthropic): ((()))(((((()(())(()(()()()(() -> () (target: ())
✅ lof_003 (google): ((()))(((((()(())(()(()()()(() -> () (target: ())
✅ lof_004 (openai): ((((())())))()((((()))((())()( -> () (target: ())
✅ lof_004 (anthropic): ((((())())))()((((()))((())()( -> () (target: ())
✅ lof_004 (google): ((((())())))()((((()))((())()( -> () (target: ())
✅ lof_005 (openai): ((()((())()(())(())))((()(())( -> () (target: ())
✅ lof_005 (anthropic): ((()((())()(())(())))((()(())( -> () (target: ())
✅ lof_005 (google): ((()((())()(())(())))((()(())( -> () (target: ())

=== 3 Failures - Full Reasoning ===

--- lof_001 (claude-opus-4-5-20251101) ---
Input: (((()(()(()(())(())(())(()))((())()(())()())()(()(())()()(())))((()())((())()()()))()(((())())()))))
Expected: void, Got: ()
Response:
<scratchpad>
Let me parse this expression carefully first by tracking the nesting levels.

(((()(()(()(())(())(())(()))((())()(())()())()(()(())()()(())))((()())((())()()()))()(((())())()))))

Let me work from the inside out, identifying opportunities to apply axioms.

First, let me find all (()) patterns which can be eliminated by I2.

Looking for (()) patterns:
- (()) appears multiple times in the expression

Let me rewrite with better structure:
(((()(()(()(())(())(())(()))((())()(())()())()(...

--- lof_001 (gemini-3-pro-preview) ---
Input: (((()(()(()(())(())(())(()))((())()(())()())()(()(())()()(())))((()())((())()()()))()(((())())()))))
Expected: void, Got: ()
Response:
<scratchpad>
I will simplify the expression by applying the Laws of Form axioms I1 (condensing marks) and I2 (canceling nested marks) from the innermost structures outward.
1.  Identify the main sub-expressions T2, T3, M, and T4 within the structure `(( T2 T3 M T4 ))`.
2.  Recursively simplify the contents of T2, T3, and T4.
3.  T2's inner components reduce to `void`, making T2 `(())`, which becomes `void`.
4.  T3 reduces to `()`. M is `()`. T4 reduces to `()`.
5.  The list `T2 T3 M T4` becomes ...

--- lof_002 (claude-opus-4-5-20251101) ---
Input: (()(())(((((()(((()()()(())(())))()(((())()(())(())(())())((())()(())(())(()))))(((()(())(()))((())()(())(())()())((())()(())(())()(())))(((())(())(())(()))((())(()))((())()()))(((())(())(()))))))))))
Expected: void, Got: ()
Response:
<scratchpad>
Let me work through this step by step. First, let me parse the expression and understand its structure.

(()(())(((((()(((()()()(())(())))()(((())()(())(())(())())((())()(())(())(()))))(((()(())(()))((())()(())(())()())((())()(())(())()(())))(((())(())(())(()))((())(()))((())()()))(((())(())(()))))))))))

Let me start simplifying from the inside out.

First, I see ()()() which can be simplified to () by I1.
I see (()) which can be simplified to void by I2.

Let me work through this ...
#+end_example

* Set up Analysis 

#+begin_src python
def analyze_results(results: List[dict], validate_traces: bool = True) -> dict:
    """Compute accuracy metrics from results, including trace validation."""
    from collections import defaultdict

    # Overall accuracy by model
    by_model = defaultdict(lambda: {"correct": 0, "total": 0, "trace_scores": []})
    for r in results:
        by_model[r["model"]]["total"] += 1
        if r["correct"]:
            by_model[r["model"]]["correct"] += 1

        # Validate traces if requested
        if validate_traces and r.get("response"):
            trace_result = validate_trace(r["response"], r["input"])
            r["trace_validation"] = trace_result
            if trace_result["steps_parsed"] > 0:
                by_model[r["model"]]["trace_scores"].append(trace_result["trace_score"])

    # Accuracy by difficulty
    by_diff = defaultdict(lambda: defaultdict(lambda: {"correct": 0, "total": 0}))
    for r in results:
        by_diff[r["model"]][r["difficulty"]]["total"] += 1
        if r["correct"]:
            by_diff[r["model"]][r["difficulty"]]["correct"] += 1

    # Failure examples
    failures = [r for r in results if not r["correct"]]

    # Error type breakdown
    error_counts = defaultdict(lambda: defaultdict(int))
    for r in results:
        if "trace_validation" in r:
            for step_num, error_type, before, after, axiom in r["trace_validation"]["errors"]:
                error_counts[r["model"]][error_type] += 1

    return {
        "by_model": {m: {
            "accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
            "correct": d["correct"],
            "total": d["total"],
            "avg_trace_score": sum(d["trace_scores"])/len(d["trace_scores"]) if d["trace_scores"] else 0
        } for m, d in by_model.items()},
        "by_difficulty": {m: {diff: {"accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
                                     "correct": d["correct"], "total": d["total"]}
                              for diff, d in diffs.items()}
                          for m, diffs in by_diff.items()},
        "error_types": dict(error_counts),
        "failures": failures[:10]
    }


# Analyze sample results (or load from file for full results)
if 'sample_results' in dir() and sample_results:
    analysis = analyze_results(sample_results)

    print("=== Accuracy by Model ===")
    for model, stats in analysis["by_model"].items():
        pct = stats["accuracy"] * 100
        trace_pct = stats["avg_trace_score"] * 100
        print(f"{model}: {pct:.1f}% final answer, {trace_pct:.1f}% trace accuracy")

    print("\n=== Accuracy by Difficulty ===")
    for model, diffs in analysis["by_difficulty"].items():
        print(f"\n{model}:")
        for diff, stats in sorted(diffs.items()):
            pct = stats["accuracy"] * 100
            print(f"  {diff}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

    if analysis["error_types"]:
        print("\n=== Trace Error Types ===")
        for model, errors in analysis["error_types"].items():
            print(f"\n{model}:")
            for error_type, count in sorted(errors.items(), key=lambda x: -x[1]):
                print(f"  {error_type}: {count}")

    if analysis["failures"]:
        print("\n=== Sample Failures ===")
        for f in analysis["failures"][:3]:
            print(f"\n{f['model']}: {f['input'][:50]}...")
            print(f"  Expected: {f['target']}, Got: {f['extracted_answer']}")
            if "trace_validation" in f:
                tv = f["trace_validation"]
                print(f"  Trace: {tv['steps_valid']}/{tv['steps_parsed']} steps valid")
                if tv["errors"]:
                    step, err, before, after, axiom = tv["errors"][0]
                    print(f"  First error (step {step}): {err} - {before[:30]}... [{axiom}]")
#+end_src

#+RESULTS:
#+begin_example
=== Accuracy by Model ===
gpt-5.1: 100.0% final answer, 28.7% trace accuracy
claude-opus-4-5-20251101: 60.0% final answer, 7.1% trace accuracy
gemini-3-pro-preview: 80.0% final answer, 29.2% trace accuracy

=== Accuracy by Difficulty ===

gpt-5.1:
  2. medium: 100.0% (2/2)
  3. hard: 100.0% (1/1)
  4. lunatic: 100.0% (1/1)
  5. extra: 100.0% (1/1)

claude-opus-4-5-20251101:
  2. medium: 100.0% (2/2)
  3. hard: 0.0% (0/1)
  4. lunatic: 100.0% (1/1)
  5. extra: 0.0% (0/1)

gemini-3-pro-preview:
  2. medium: 100.0% (2/2)
  3. hard: 0.0% (0/1)
  4. lunatic: 100.0% (1/1)
  5. extra: 100.0% (1/1)

=== Trace Error Types ===

gpt-5.1:
  wrong_result: 13
  wrong_axiom: 5
  scope_collapse: 1

claude-opus-4-5-20251101:
  malformed: 13
  wrong_result: 1

gemini-3-pro-preview:
  wrong_result: 13
  malformed: 7
  wrong_axiom: 7

=== Sample Failures ===

claude-opus-4-5-20251101: (((()(()(()(())(())(())(()))((())()(())()())()(()(...
  Expected: void, Got: ()
  Trace: 0/5 steps valid
  First error (step 1): malformed - (((()(()(()(())(())(())(()))((... [I2]

gemini-3-pro-preview: (((()(()(()(())(())(())(()))((())()(())()())()(()(...
  Expected: void, Got: ()
  Trace: 0/10 steps valid
  First error (step 1): malformed - (((()(()(()(())(())(())(()))((... [I2]

claude-opus-4-5-20251101: (()(())(((((()(((()()()(())(())))()(((())()(())(()...
  Expected: void, Got: ()
  Trace: 0/2 steps valid
  First error (step 1): malformed - (()(())(((((()(((()()()(())(()... [I1]
#+end_example

* Batch API: Prepare Jobs

Submit batch jobs to all three providers for the full evaluation.

Uses an append-only JSONL log for robustness - never overwrites, easy to recover.

#+begin_src python

import tempfile
from openai import OpenAI
from anthropic import Anthropic
from google import genai

BATCH_LOG = "batch_log.jsonl"


def log_batch_event(event: str, provider: str, batch_id: str, **data):
    """Append an event to the batch log."""
    entry = {
        "timestamp": datetime.now().isoformat(),
        "event": event,
        "provider": provider,
        "batch_id": batch_id,
        ,**data
    }
    with open(BATCH_LOG, "a") as f:
        f.write(json.dumps(entry) + "\n")


def get_latest_batches() -> dict:
    """Read log and return most recent batch per provider (that isn't completed)."""
    batches = {}  # provider -> {batch_id, model, status, ...}
    try:
        with open(BATCH_LOG, "r") as f:
            for line in f:
                if not line.strip():
                    continue
                entry = json.loads(line)
                provider = entry["provider"]
                if entry["event"] == "submitted":
                    batches[provider] = {
                        "batch_id": entry["batch_id"],
                        "model": entry.get("model"),
                        "status": "submitted"
                    }
                elif entry["event"] == "status":
                    if provider in batches and batches[provider]["batch_id"] == entry["batch_id"]:
                        batches[provider]["status"] = entry["status"]
                elif entry["event"] == "completed":
                    if provider in batches and batches[provider]["batch_id"] == entry["batch_id"]:
                        batches[provider]["status"] = "completed"
                        batches[provider]["results"] = entry.get("results", [])
    except FileNotFoundError:
        pass
    return batches


def create_openai_batch(cases: List[dict], model: str = MODELS["openai"]) -> str:
    """Create and submit OpenAI batch job. Returns batch_id."""
    client = OpenAI()

    # Create JSONL file
    jsonl_content = ""
    for case in cases:
        prompt = PROMPT_TEMPLATE.format(expression=case["input"])
        request = {
            "custom_id": case["id"],
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0
            }
        }
        jsonl_content += json.dumps(request) + "\n"

    # Upload file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        f.write(jsonl_content)
        temp_path = f.name

    with open(temp_path, 'rb') as f:
        file_obj = client.files.create(file=f, purpose="batch")

    # Create batch
    batch = client.batches.create(
        input_file_id=file_obj.id,
        endpoint="/v1/chat/completions",
        completion_window="24h"
    )
    return batch.id


def create_anthropic_batch(cases: List[dict], model: str = MODELS["anthropic"]) -> str:
    """Create and submit Anthropic batch job. Returns batch_id."""
    client = Anthropic()

    requests_list = []
    for case in cases:
        prompt = PROMPT_TEMPLATE.format(expression=case["input"])
        requests_list.append({
            "custom_id": case["id"],
            "params": {
                "model": model,
                "max_tokens": 1024,
                "messages": [{"role": "user", "content": prompt}]
            }
        })

    batch = client.messages.batches.create(requests=requests_list)
    return batch.id


def create_gemini_batch(cases: List[dict], model: str = MODELS["google"]) -> str:
    """Create Gemini batch job using SDK. Returns batch job name."""
    client = genai.Client()

    # Build inline requests
    inline_requests = []
    for case in cases:
        prompt = PROMPT_TEMPLATE.format(expression=case["input"])
        inline_requests.append({
            "contents": [{"parts": [{"text": prompt}], "role": "user"}]
        })

    batch_job = client.batches.create(
        model=f"models/{model}",
        src=inline_requests,
        config={"display_name": f"lof-eval-{datetime.now().strftime('%Y%m%d-%H%M%S')}"},
    )
    return batch_job.name
    
#+end_src

#+RESULTS:

* Submit Jobs
#+begin_src python
# Submit all batches (appends to log, never overwrites)
print(f"Submitting batches for {len(test_cases)} test cases...")

if OPENAI_API_KEY:
    print("  Submitting OpenAI batch...")
    try:
        batch_id = create_openai_batch(test_cases)
        log_batch_event("submitted", "openai", batch_id, model=MODELS["openai"], n_cases=len(test_cases))
        print(f"    OpenAI batch ID: {batch_id}")
    except Exception as e:
        print(f"    OpenAI batch failed: {e}")

if ANTHROPIC_API_KEY:
    print("  Submitting Anthropic batch...")
    try:
        batch_id = create_anthropic_batch(test_cases)
        log_batch_event("submitted", "anthropic", batch_id, model=MODELS["anthropic"], n_cases=len(test_cases))
        print(f"    Anthropic batch ID: {batch_id}")
    except Exception as e:
        print(f"    Anthropic batch failed: {e}")

if GEMINI_API_KEY:
    print("  Submitting Gemini batch...")
    try:
        batch_id = create_gemini_batch(test_cases)
        log_batch_event("submitted", "google", batch_id, model=MODELS["google"], n_cases=len(test_cases))
        print(f"    Gemini batch ID: {batch_id}")
    except Exception as e:
        print(f"    Gemini batch failed: {e}")

print(f"\nBatch events appended to {BATCH_LOG}")
#+end_src

#+RESULTS:
#+begin_example
Submitting batches for 500 test cases...
  Submitting OpenAI batch...
    OpenAI batch ID: batch_6939e6ddb9108190909adc4e1369f234
  Submitting Anthropic batch...
    Anthropic batch ID: msgbatch_01CnXznANgLoLuvaw7B479vY
  Submitting Gemini batch...
Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
    Gemini batch ID: batches/lhydhmxj6bn73y4dwtcmeo9g0pifta6xlwr4

Batch events appended to batch_log.jsonl
#+end_example

* Batch API: Check Status

Poll batch job status and download results when complete.

#+begin_src python
def check_openai_batch(batch_id: str) -> Tuple[str, Optional[List[dict]]]:
    """Check OpenAI batch status. Returns (status, results_or_none)."""
    client = OpenAI()
    batch = client.batches.retrieve(batch_id)
    if batch.status == "completed":
        # Download results
        output_file = client.files.content(batch.output_file_id)
        results = []
        for line in output_file.text.strip().split("\n"):
            result = json.loads(line)
            results.append({
                "id": result["custom_id"],
                "response": result["response"]["body"]["choices"][0]["message"]["content"],
                "provider": "openai"
            })
        return "completed", results
    return batch.status, None


def check_anthropic_batch(batch_id: str) -> Tuple[str, Optional[List[dict]]]:
    """Check Anthropic batch status. Returns (status, results_or_none)."""
    client = Anthropic()
    batch = client.messages.batches.retrieve(batch_id)
    if batch.processing_status == "ended":
        results = []
        for result in client.messages.batches.results(batch_id):
            results.append({
                "id": result.custom_id,
                "response": result.result.message.content[0].text,
                "provider": "anthropic"
            })
        return "completed", results
    return batch.processing_status, None


def check_gemini_batch(batch_name: str) -> Tuple[str, Optional[List[dict]]]:
    """Check Gemini batch status using SDK. Returns (status, results_or_none)."""
    client = genai.Client()
    batch = client.batches.get(name=batch_name)

    if batch.state.name != "JOB_STATE_SUCCEEDED":
        return batch.state.name.lower(), None

    # Batch complete - extract inline results
    results = []
    if batch.dest and batch.dest.inlined_responses:
        for i, inline_resp in enumerate(batch.dest.inlined_responses):
            if inline_resp.response:
                try:
                    text = inline_resp.response.text
                except AttributeError:
                    text = str(inline_resp.response)
            elif inline_resp.error:
                text = f"ERROR: {inline_resp.error}"
            else:
                text = "ERROR: No response"
            results.append({
                "id": f"lof_{i+1:03d}",  # Match original case IDs
                "response": text,
                "provider": "google"
            })

    return "completed", results


# Load latest batches from log
batches = get_latest_batches()

if not batches:
    print("No batches found in log. Run the submit cell first.")
else:
    print("Checking batch status...")
    all_complete = True

    for provider, info in batches.items():
        if info["status"] == "completed":
            print(f"  {provider}: completed (already)")
            continue

        batch_id = info["batch_id"]
        try:
            if provider == "openai" and OPENAI_API_KEY:
                status, results = check_openai_batch(batch_id)
            elif provider == "anthropic" and ANTHROPIC_API_KEY:
                status, results = check_anthropic_batch(batch_id)
            elif provider == "google" and GEMINI_API_KEY:
                status, results = check_gemini_batch(batch_id)
            else:
                print(f"  {provider}: skipped (no API key)")
                continue

            print(f"  {provider}: {status}")

            if results:
                log_batch_event("completed", provider, batch_id, results=results)
            elif status != info.get("status"):
                log_batch_event("status", provider, batch_id, status=status)

            if status != "completed":
                all_complete = False

        except Exception as e:
            print(f"  {provider}: error - {e}")
            all_complete = False

    if all_complete:
        print("\nAll batches complete! Run the next cell to process results.")
    else:
        print("\nSome batches still processing. Re-run this cell to check again.")
#+end_src

#+RESULTS:
: Checking batch status...
:   openai: in_progress
:   anthropic: in_progress
: Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.
:   google: job_state_running
: 
: Some batches still processing. Re-run this cell to check again.

* Process Batch Results

Combine batch results into unified format.

#+begin_src python
# Load completed batches from log
batches = get_latest_batches()

# Build case lookup
case_lookup = {c["id"]: c for c in test_cases}

# Process all results
all_results = []
for provider, info in batches.items():
    if info["status"] != "completed" or "results" not in info:
        print(f"Warning: No results for {provider} (status: {info['status']})")
        continue

    for r in info["results"]:
        case = case_lookup.get(r["id"])
        if not case:
            print(f"Warning: Unknown case ID {r['id']}")
            continue
        response_text = r.get("response") or ""
        answer = extract_answer(response_text)
        all_results.append({
            "id": r["id"],
            "input": case["input"],
            "target": case["target"],
            "difficulty": case["difficulty"],
            "depth": case.get("depth", 0),
            "provider": provider,
            "model": info["model"],
            "response": response_text,
            "extracted_answer": answer,
            "correct": answer == case["target"]
        })

# Save full results
output = {
    "metadata": {
        "timestamp": datetime.now().isoformat(),
        "n_cases": len(test_cases),
        "prompt_template": PROMPT_TEMPLATE
    },
    "results": all_results
}

with open("results.json", "w") as f:
    json.dump(output, f, indent=2)

print(f"Saved {len(all_results)} results to results.json")
#+end_src

#+RESULTS:
: Saved 90 results to results.json

* Load and Analyze Saved Results

#+begin_src python
# Load previously saved results
with open("results.json", "r") as f:
    saved = json.load(f)

print(f"Loaded {len(saved['results'])} results from {saved['metadata']['timestamp']}")
analysis = analyze_results(saved['results'])

# Print full analysis
print("\n=== FULL RESULTS ===\n")
print("=== Accuracy by Model ===")
for model, stats in analysis["by_model"].items():
    pct = stats["accuracy"] * 100
    trace_pct = stats["avg_trace_score"] * 100
    print(f"{model}: {pct:.1f}% final answer, {trace_pct:.1f}% trace accuracy")

print("\n=== Accuracy by Difficulty ===")
for model, diffs in analysis["by_difficulty"].items():
    print(f"\n{model}:")
    for diff, stats in sorted(diffs.items()):
        pct = stats["accuracy"] * 100
        print(f"  {diff}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

if analysis["error_types"]:
    print("\n=== Trace Error Types ===")
    for model, errors in analysis["error_types"].items():
        print(f"\n{model}:")
        for error_type, count in sorted(errors.items(), key=lambda x: -x[1]):
            print(f"  {error_type}: {count}")

if analysis["failures"]:
    print("\n=== Sample Failures ===")
    for f in analysis["failures"][:3]:
        print(f"\n{f['model']}: {f['input'][:60]}...")
        print(f"  Expected: {f['target']}, Got: {f['extracted_answer']}")
        if "trace_validation" in f:
            tv = f["trace_validation"]
            print(f"  Trace: {tv['steps_valid']}/{tv['steps_parsed']} steps valid")
            if tv["errors"]:
                step, err, before, after, axiom = tv["errors"][0]
                print(f"  First error (step {step}): {err}")
#+end_src

#+RESULTS:
#+begin_example
Loaded 90 results from 2025-12-07T11:53:36.408677

=== FULL RESULTS ===

=== Accuracy by Model ===
gpt-5.1: 53.3% final answer, 0.0% trace accuracy
claude-opus-4-5-20251101: 63.3% final answer, 0.0% trace accuracy
gemini-3-pro-preview: 66.7% final answer, 0.0% trace accuracy

=== Accuracy by Difficulty ===

gpt-5.1:
  1. easy: 90.0% (9/10)
  2. medium: 40.0% (4/10)
  3. hard: 30.0% (3/10)

claude-opus-4-5-20251101:
  1. easy: 100.0% (10/10)
  2. medium: 60.0% (6/10)
  3. hard: 30.0% (3/10)

gemini-3-pro-preview:
  1. easy: 90.0% (9/10)
  2. medium: 60.0% (6/10)
  3. hard: 50.0% (5/10)

=== Sample Failures ===

gpt-5.1: ((((()()(())((((((())(())()())(()()(())(())(()))((())(())(()...
  Expected: (), Got: void
  Trace: 0/0 steps valid

gpt-5.1: ((((((((((())()(()))()(()(())))((()(())(())(())(()))(()())((...
  Expected: void, Got: ()
  Trace: 0/0 steps valid

gpt-5.1: ((())((((())()(())))(((())(()))((())(())(()))((())(())(())))...
  Expected: (), Got: void
  Trace: 0/0 steps valid
#+end_example

#+title: Laws of Form Arithmetic Evaluation Suite
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python ".venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name    | Rule           | Interpretation      |
|-------+---------+----------------+---------------------|
| I1    | Number  | =()()= → =()=  | Condense/Confirm    |
| I2    | Order   | =(())= → void  | Cancel/Compensate   |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (the mark) — equivalent to TRUE or 1
- void (empty) — equivalent to FALSE or 0

* Install Packages
#+begin_src bash
uv sync
#+end_src

* Setup

#+begin_src python
import os
import json
import random
import re
from datetime import datetime
from typing import Tuple, List, Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# API clients (we'll initialize these when needed)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

print("Setup complete.")
print(f"OpenAI key: {'set' if OPENAI_API_KEY else 'missing'}")
print(f"Anthropic key: {'set' if ANTHROPIC_API_KEY else 'missing'}")
print(f"Google key: {'set' if GOOGLE_API_KEY else 'missing'}")
#+end_src

#+RESULTS:
: Setup complete.
: OpenAI key: set
: Anthropic key: set
: Google key: set

* Form Representation

We use two representations:
- *Internal*: Nested Python lists. =[[]]= represents =(())=, =[[], []]= represents =()()=
- *String*: Parentheses notation for display and LLM prompts

#+begin_src python
def form_to_string(form: list) -> str:
    """Convert internal form representation to string."""
    if not form:
        return ""
    return "".join(f"({form_to_string(child)})" for child in form)

def string_to_form(s: str) -> list:
    """Parse string notation to internal form."""
    result = []
    i = 0
    while i < len(s):
        if s[i] == '(':
            # Find matching close paren
            depth = 1
            j = i + 1
            while j < len(s) and depth > 0:
                if s[j] == '(':
                    depth += 1
                elif s[j] == ')':
                    depth -= 1
                j += 1
            # Recursively parse contents
            result.append(string_to_form(s[i+1:j-1]))
            i = j
        else:
            i += 1
    return result

# Test
test_forms = ["()", "(())", "()()", "((()))", "(()())"]
for s in test_forms:
    f = string_to_form(s)
    back = form_to_string(f)
    print(f"{s:12} -> {str(f):20} -> {back}")
#+end_src

#+RESULTS:
: ()           -> [[]]                 -> ()
: (())         -> [[[]]]               -> (())
: ()()         -> [[], []]             -> ()()
: ((()))       -> [[[[]]]]             -> ((()))
: (()())       -> [[[], []]]           -> (()())

* Form Generator

Generate random well-formed LoF expressions with controlled complexity.

#+begin_src python
def form_depth(form: list) -> int:
    """Calculate the nesting depth of a form."""
    if not form:
        return 0
    return 1 + max(form_depth(child) for child in form)


def generate_form(min_depth: int = 1, max_depth: int = 3, max_width: int = 3, depth: int = 0) -> list:
    """
    Generate a random LoF form with guaranteed minimum depth.

    Args:
        min_depth: Minimum nesting depth (guaranteed)
        max_depth: Maximum nesting depth
        max_width: Maximum number of adjacent forms at any level
        depth: Current depth (internal use)

    Returns:
        A form as nested lists
    """
    remaining_min = min_depth - depth
    remaining_max = max_depth - depth

    if remaining_max <= 0:
        # At max depth, return empty (void) or single mark
        return [[]] if random.random() > 0.5 else []

    if remaining_min > 0:
        # Must continue building to meet min_depth
        width = random.randint(1, max_width)  # At least 1
    else:
        # Can optionally terminate
        width = random.randint(0, max_width)
        if width == 0:
            return []

    result = []
    # Ensure at least one child reaches min_depth
    guaranteed_deep = random.randint(0, width - 1) if remaining_min > 0 else -1

    for i in range(width):
        if i == guaranteed_deep:
            # This child must reach min_depth
            child_form = generate_form(min_depth, max_depth, max_width, depth + 1)
        else:
            # This child can be shallower
            child_form = generate_form(0, max_depth, max_width, depth + 1)
        result.append(child_form)

    return result


# Generate some examples
random.seed(1010101423)
print("Sample generated forms (min_depth=2, max_depth=3):")
for i in range(10):
    form = generate_form(min_depth=2, max_depth=3, max_width=2)
    s = form_to_string(form)
    d = form_depth(form)
    print(f"  {i+1:2}. depth={d}: {s if s else '<void>'}")
#+end_src

#+RESULTS:
#+begin_example
Sample generated forms (min_depth=2, max_depth=3):
   1. depth=4: ((())((())))
   2. depth=4: (((()))(()(())))
   3. depth=4: ((())((())))
   4. depth=4: (((())))((()))
   5. depth=2: (()())
   6. depth=4: (((())()))
   7. depth=4: (((())))
   8. depth=3: ((()))((()())(()))
   9. depth=4: (((())())(()))
  10. depth=4: ((()(()))((())()))(((())(())))
#+end_example

* Ground-Truth Simplifier

Apply I1 and I2 exhaustively until fixed point, tracking each step.

#+begin_src python
def simplify_step(form: list) -> Tuple[list, Optional[str]]:
    """
    Apply one simplification step if possible.

    Returns:
        (new_form, axiom_applied) where axiom_applied is None if no change
    """
    # I1 (Number): IDENTICAL adjacent forms condense (aa = a)
    # Only applies when ALL adjacent forms are equal
    if len(form) > 1 and all(child == form[0] for child in form):
        return [form[0]], "I1"

    # I2 (Order): (()) = void
    # A mark is (()) if its content is [[]] (representing "()")
    # Check each top-level mark and remove any that are (())
    for i, child in enumerate(form):
        if child == [[]]:
            # This mark is (()) = void, remove it
            new_form = form[:i] + form[i+1:]
            return new_form, "I2"

    # Recursively check children
    for i, child in enumerate(form):
        new_child, axiom = simplify_step(child)
        if axiom:
            new_form = form[:i] + [new_child] + form[i+1:]
            return new_form, axiom

    return form, None


def simplify(form: list) -> Tuple[list, List[Tuple[str, str]]]:
    """
    Fully simplify a form to canonical form.

    Returns:
        (canonical_form, steps) where steps is list of (form_string, axiom)
    """
    steps = []
    current = form

    while True:
        new_form, axiom = simplify_step(current)
        if axiom is None:
            break
        steps.append((form_to_string(current), axiom))
        current = new_form

    return current, steps


def canonical_string(form: list) -> str:
    """Get the canonical string representation."""
    simplified, _ = simplify(form)
    s = form_to_string(simplified)
    return s if s else "void"


# Test the simplifier
print("Simplification examples:")
test_exprs = ["()()", "(())", "((()))", "(()())", "()(())", "(()(()))", "(())()", "(())(())"]
for expr in test_exprs:
    form = string_to_form(expr)
    result, steps = simplify(form)
    result_str = form_to_string(result) if result else "void"
    print(f"\n{expr} -> {result_str}")
    for step_form, axiom in steps:
        print(f"  {step_form} [{axiom}]")
#+end_src

#+RESULTS:
#+begin_example
Simplification examples:

()() -> ()
  ()() [I1]

(()) -> void
  (()) [I2]

((())) -> ()
  ((())) [I2]

(()()) -> void
  (()()) [I1]
  (()) [I2]

()(()) -> ()
  ()(()) [I2]

(()(())) -> void
  (()(())) [I2]
  (()) [I2]

(())() -> ()
  (())() [I2]

(())(()) -> void
  (())(()) [I1]
  (()) [I2]
#+end_example

* Test Case Generation

Generate a reproducible test suite across difficulty levels.

Difficulty is defined by *minimum* depth:
- Easy: depth 1-2 (simple marks and single nesting)
- Medium: depth 2-3 (requires at least one I2 application)
- Hard: depth 3-4 (multiple nested reductions)

#+begin_src python
def generate_test_cases(n: int = 100, seed: int = 2024) -> List[dict]:
    """Generate n test cases with varying difficulty."""
    random.seed(seed)
    cases = []

    # Distribution with (difficulty, min_depth, max_depth, max_width)
    # min_depth guarantees complexity for each difficulty level
    difficulties = (
        [("1. easy", 1, 2, 2)] * 30 +
        [("2. medium", 2, 3, 2)] * 40 +
        [("3. hard", 3, 4, 3)] * 30
    )
    random.shuffle(difficulties)

    for i, (diff, min_d, max_d, max_w) in enumerate(difficulties[:n]):
        form = generate_form(min_depth=min_d, max_depth=max_d, max_width=max_w)
        input_str = form_to_string(form) if form else "void"
        depth = form_depth(form)

        target = canonical_string(form)
        _, steps = simplify(form)

        cases.append({
            "id": f"lof_{i+1:03d}",
            "input": input_str,
            "target": target,
            "difficulty": diff,
            "depth": depth,
            "steps": len(steps)
        })

    return cases


# Generate and preview test cases
test_cases = generate_test_cases(100)
print(f"Generated {len(test_cases)} test cases\n")

# Preview distribution
from collections import Counter
diff_counts = Counter(c["difficulty"] for c in test_cases)
target_counts = Counter(c["target"] for c in test_cases)
depth_by_diff = {}
for c in test_cases:
    depth_by_diff.setdefault(c["difficulty"], []).append(c["depth"])

print("By difficulty:")
for diff in ["easy", "medium", "hard"]:
    depths = depth_by_diff.get(diff, [])
    avg_depth = sum(depths) / len(depths) if depths else 0
    print(f"  {diff}: {diff_counts[diff]} cases, avg depth={avg_depth:.1f}, range=[{min(depths)}-{max(depths)}]")

print("\nBy target value:")
for target, count in sorted(target_counts.items()):
    print(f"  {target}: {count}")

print("\nSample cases:")
for case in test_cases[:5]:
    print(f"  {case['id']}: {case['input']:20} -> {case['target']:6} (d={case['depth']}, {case['difficulty']}, {case['steps']} steps)")
#+end_src

#+RESULTS:
#+begin_example
Generated 100 test cases

By difficulty:
  easy: 30 cases, avg depth=2.3, range=[1-3]
  medium: 40 cases, avg depth=3.5, range=[2-4]
  hard: 30 cases, avg depth=4.8, range=[3-5]

By target value:
  (): 63
  void: 37

Sample cases:
  lof_001: (()())((())(()))     -> ()     (d=3, easy, 4 steps)
  lof_002: ((()))(((())(()))(())) -> ()     (d=4, medium, 5 steps)
  lof_003: ((()((())(())(())))) -> ()     (d=5, hard, 4 steps)
  lof_004: (())                 -> void   (d=2, medium, 1 steps)
  lof_005: (((())()))           -> ()     (d=4, medium, 2 steps)
#+end_example

* Save Test Cases

#+begin_src python
# Save test cases for reproducibility
with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
print(f"Saved {len(test_cases)} test cases to test_cases.json")
#+end_src

#+RESULTS:
: Saved 100 test cases to test_cases.json

* LLM Clients

Initialize API clients for each provider.

#+begin_src python
# OpenAI client
openai_client = None
if OPENAI_API_KEY:
    import openai
    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    print("OpenAI client initialized")

# Anthropic client
anthropic_client = None
if ANTHROPIC_API_KEY:
    import anthropic
    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    print("Anthropic client initialized")

# Google client (using newer google.genai SDK for batch support)
google_client = None
google_model = None
if GOOGLE_API_KEY:
    from google import genai
    google_client = genai.Client(api_key=GOOGLE_API_KEY)
    google_model = google_client.models  # For non-batch calls
    print("Google Gemini client initialized")
    
#+end_src

#+RESULTS:
: OpenAI client initialized
: Anthropic client initialized
: Google Gemini client initialized

* Async Clients

Initialize async clients for parallel execution.

#+begin_src python
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
import asyncio

async_openai = AsyncOpenAI() if OPENAI_API_KEY else None
async_anthropic = AsyncAnthropic() if ANTHROPIC_API_KEY else None

# Google's genai library uses generate_content_async on the model directly
# async_google uses the same google_model initialized above

print("Async clients initialized.")
#+end_src

#+RESULTS:
: Async clients initialized.

* Eval Functions

#+begin_src python
PROMPT_TEMPLATE = """Simplify this Laws of Form expression using the two axioms:

I1 (Number): ()() = ()
   Multiple adjacent marks condense to a single mark.

I2 (Order): (()) = void
   A mark containing only a mark cancels to void (nothing).

Expression: {expression}

Apply the axioms step by step until you reach either:
- () (the mark)
- void (empty/nothing)

Show your reasoning, then state your final answer on the last line as either:
FINAL: ()
or
FINAL: void
"""


def extract_answer(response: str) -> str:
    """Extract the final answer from LLM response."""
    # Look for FINAL: pattern
    match = re.search(r'FINAL:\s*(\(\)|void)', response, re.IGNORECASE)
    if match:
        ans = match.group(1).lower()
        return "()" if ans == "()" else "void"

    # Fallback: look for last occurrence of () or void
    response_lower = response.lower()
    last_mark = response_lower.rfind("()")
    last_void = max(response_lower.rfind("void"), response_lower.rfind("empty"))

    if last_mark > last_void:
        return "()"
    elif last_void > last_mark:
        return "void"

    return "unknown"


def eval_openai(expression: str, model: str = "gpt-5.1") -> Tuple[str, str]:
    """Evaluate using OpenAI API."""
    if not openai_client:
        return "", "no_client"

    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    text = response.choices[0].message.content
    return text, extract_answer(text)

def eval_anthropic(expression: str, model: str = "claude-sonnet-4-5-20250929") -> Tuple[str, str]:
    """Evaluate using Anthropic API."""
    if not anthropic_client:
        return "", "no_client"

    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = anthropic_client.messages.create(
        model=model,
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    text = response.content[0].text
    return text, extract_answer(text)


def eval_google(expression: str) -> Tuple[str, str]:
    """Evaluate using Google Gemini API."""
    if not google_model:
        return "", "no_client"

    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = google_model.generate_content(prompt)
    text = response.text
    return text, extract_answer(text)


print("Eval functions defined.")
#+end_src

#+RESULTS:
: Eval functions defined.

* Async Eval Functions

Async versions for parallel sample runs.

#+begin_src python
async def eval_openai_async(expression: str, model: str = "gpt-5.1") -> Tuple[str, str, str]:
    """Evaluate using OpenAI API (async). Returns (response, answer, model)."""
    if not async_openai:
        return "", "no_client", model
    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = await async_openai.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    text = response.choices[0].message.content
    return text, extract_answer(text), model


async def eval_anthropic_async(expression: str, model: str = "claude-sonnet-4-5-20250929") -> Tuple[str, str, str]:
    """Evaluate using Anthropic API (async). Returns (response, answer, model)."""
    if not async_anthropic:
        return "", "no_client", model
    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = await async_anthropic.messages.create(
        model=model,
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    text = response.content[0].text
    return text, extract_answer(text), model


async def eval_google_async(expression: str, model: str = "gemini-2.5-pro") -> Tuple[str, str, str]:
    """Evaluate using Google Gemini API (async). Returns (response, answer, model)."""
    if not google_model:
        return "", "no_client", model
    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = await google_model.generate_content_async(prompt)
    text = response.text
    return text, extract_answer(text), model


print("Async eval functions defined.")
#+end_src

#+RESULTS:
: Async eval functions defined.

* Sample Run (Async)

Run a small subset with async for quick iteration. Saves full outputs.

#+begin_src python
async def run_sample_async(cases: List[dict], n: int = 5) -> List[dict]:
    """Run async evaluation on sample cases."""
    sample = cases[:n]
    results = []

    # Build list of all (case, provider, eval_fn) combinations
    tasks = []
    task_info = []  # Track which case/provider each task belongs to

    for case in sample:
        if async_openai:
            tasks.append(eval_openai_async(case["input"]))
            task_info.append((case, "openai"))
        if async_anthropic:
            tasks.append(eval_anthropic_async(case["input"]))
            task_info.append((case, "anthropic"))
        if google_model:
            tasks.append(eval_google_async(case["input"]))
            task_info.append((case, "google"))

    # Run all in parallel
    responses = await asyncio.gather(*tasks, return_exceptions=True)

    # Process results
    for (case, provider), resp in zip(task_info, responses):
        if isinstance(resp, Exception):
            response, answer, model = f"ERROR: {resp}", "error", "unknown"
        else:
            response, answer, model = resp

        results.append({
            "id": case["id"],
            "input": case["input"],
            "target": case["target"],
            "difficulty": case["difficulty"],
            "depth": case.get("depth", 0),
            "provider": provider,
            "model": model,
            "response": response,
            "extracted_answer": answer,
            "correct": answer == case["target"]
        })

    return results


# Run sample evaluation
print("Running async sample evaluation...")
sample_results = asyncio.run(run_sample_async(test_cases, n=5))

# Save full results
with open("sample_results.json", "w") as f:
    json.dump(sample_results, f, indent=2)
print(f"Saved {len(sample_results)} results to sample_results.json")

# Display summary
print("\n=== Sample Results ===")
for r in sample_results:
    status = "✅" if r["correct"] else "❌"
    print(f"{status} {r['id']} ({r['provider']}): {r['input'][:30]:30} -> {r['extracted_answer']} (target: {r['target']})")

# Show failures with reasoning
failures = [r for r in sample_results if not r["correct"]]
if failures:
    print(f"\n=== {len(failures)} Failures - Full Reasoning ===")
    for f in failures[:3]:
        print(f"\n--- {f['id']} ({f['model']}) ---")
        print(f"Input: {f['input']}")
        print(f"Expected: {f['target']}, Got: {f['extracted_answer']}")
        print(f"Response:\n{f['response'][:500]}...")
#+end_src

#+RESULTS:
#+begin_example
Running async sample evaluation...
Saved 15 results to sample_results.json

=== Sample Results ===
✓ lof_001 (openai): (()())((())(()))               -> () (target: ())
✗ lof_001 (anthropic): (()())((())(()))               -> void (target: ())
✗ lof_001 (google): (()())((())(()))               -> void (target: ())
✓ lof_002 (openai): ((()))(((())(()))(()))         -> () (target: ())
✗ lof_002 (anthropic): ((()))(((())(()))(()))         -> void (target: ())
✗ lof_002 (google): ((()))(((())(()))(()))         -> void (target: ())
✓ lof_003 (openai): ((()((())(())(()))))           -> () (target: ())
✓ lof_003 (anthropic): ((()((())(())(()))))           -> () (target: ())
✗ lof_003 (google): ((()((())(())(()))))           -> void (target: ())
✓ lof_004 (openai): (())                           -> void (target: void)
✓ lof_004 (anthropic): (())                           -> void (target: void)
✓ lof_004 (google): (())                           -> void (target: void)
✗ lof_005 (openai): (((())()))                     -> void (target: ())
✗ lof_005 (anthropic): (((())()))                     -> void (target: ())
✗ lof_005 (google): (((())()))                     -> void (target: ())

=== 8 Failures - Full Reasoning ===

--- lof_001 (claude-sonnet-4-5-20250929) ---
Input: (()())((())(()))
Expected: (), Got: void
Response:
I'll simplify this expression step by step using the two axioms.

Starting expression: (()())((())(()))

Let me work on each part systematically.

,**Step 1:** Look at the first part: (()())
- Inside we have ()() 
- By I1 (Number): ()() = ()
- So (()()) = (())

,**Step 2:** Apply I2 to (())
- By I2 (Order): (()) = void
- So the first part becomes: void

,**Step 3:** Look at the second part: ((())(()))
- Let's simplify what's inside: (())(())
- First, by I2: (()) = void, so we have (void)(void)
- Th...

--- lof_001 (gemini-2.5-pro) ---
Input: (()())((())(()))
Expected: (), Got: void
Response:
Let's simplify the expression `(()())((())(()))` step-by-step using the given axioms:

,**Axioms:**
I1 (Number): `()()` = `()`
I2 (Order): `(())` = `void`

,**Expression:** `(()())((())(()))`

,**Step 1: Simplify the first main component `(()())`**
   `(()())`
   Look inside the outermost mark: `()()`
   Apply Axiom I1 to `()()`: `()()` becomes `()`
   So, `(()())` becomes `(())`
   Apply Axiom I2 to `(())`: `(())` becomes `void`

,**Step 2: Substitute the simplified first component back into the ex...

--- lof_002 (claude-sonnet-4-5-20250929) ---
Input: ((()))(((())(()))(()))
Expected: (), Got: void
Response:
I'll simplify this expression step by step using the two axioms.

,**Starting expression:** ((()))(((())(()))(()))

Let me work from the innermost parentheses outward.

,**Step 1:** Identify innermost expressions
- (()) appears multiple times
- By I2 (Order): (()) = void

,**Step 2:** Apply I2 to all instances of (())
((()))(((**void**)(**void**))(**void**))

This becomes:
((()))((**void void**)**void**)

,**Step 3:** Simplify ((**void void**)**void**)
The inner part has: **void void** which is just...
#+end_example

* Batch API: Submit Jobs

Submit batch jobs to all three providers for the full evaluation.

#+begin_src python
import tempfile
import requests

def create_openai_batch(cases: List[dict], model: str = "gpt-5.1") -> str:
    """Create and submit OpenAI batch job. Returns batch_id."""
    # Create JSONL file
    jsonl_content = ""
    for case in cases:
        prompt = PROMPT_TEMPLATE.format(expression=case["input"])
        request = {
            "custom_id": case["id"],
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0
            }
        }
        jsonl_content += json.dumps(request) + "\n"

    # Upload file
    with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
        f.write(jsonl_content)
        temp_path = f.name

    with open(temp_path, 'rb') as f:
        file_obj = openai_client.files.create(file=f, purpose="batch")

    # Create batch
    batch = openai_client.batches.create(
        input_file_id=file_obj.id,
        endpoint="/v1/chat/completions",
        completion_window="24h"
    )
    return batch.id


def create_anthropic_batch(cases: List[dict], model: str = "claude-sonnet-4-5-20250929") -> str:
    """Create and submit Anthropic batch job. Returns batch_id."""
    requests_list = []
    for case in cases:
        prompt = PROMPT_TEMPLATE.format(expression=case["input"])
        requests_list.append({
            "custom_id": case["id"],
            "params": {
                "model": model,
                "max_tokens": 1024,
                "messages": [{"role": "user", "content": prompt}]
            }
        })

    batch = anthropic_client.messages.batches.create(requests=requests_list)
    return batch.id


def create_gemini_batch(cases: List[dict], model: str = "gemini-2.5-pro") -> str:
    """Create Gemini batch job using SDK. Returns batch job name."""
    if not google_client:
        raise RuntimeError("Google client not initialized")

    # Build inline requests
    inline_requests = []
    for case in cases:
        prompt = PROMPT_TEMPLATE.format(expression=case["input"])
        inline_requests.append({
            "contents": [{"parts": [{"text": prompt}], "role": "user"}]
        })

    batch_job = google_client.batches.create(
        model=f"models/{model}",
        src=inline_requests,
        config={"display_name": f"lof-eval-{datetime.now().strftime('%Y%m%d-%H%M%S')}"},
    )
    return batch_job.name


# Submit all batches
batch_status = {"timestamp": datetime.now().isoformat(), "batches": {}}

# if openai_client:
#     print("Submitting OpenAI batch...")
#     batch_status["batches"]["openai"] = {
#         "id": create_openai_batch(test_cases),
#         "model": "gpt-5.1",
#         "status": "submitted"
#     }
#     print(f"  OpenAI batch ID: {batch_status['batches']['openai']['id']}")

# if anthropic_client:
#     print("Submitting Anthropic batch...")
#     batch_status["batches"]["anthropic"] = {
#         "id": create_anthropic_batch(test_cases),
#         "model": "claude-sonnet-4-5-20250929",
#         "status": "submitted"
#     }
#     print(f"  Anthropic batch ID: {batch_status['batches']['anthropic']['id']}")

if GOOGLE_API_KEY:
    print("Submitting Gemini batch...")
    try:
        batch_status["batches"]["google"] = {
            "id": create_gemini_batch(test_cases),
            "model": "gemini-2.5-pro",
            "status": "submitted"
        }
        print(f"  Gemini batch ID: {batch_status['batches']['google']['id']}")
    except Exception as e:
        print(f"  Gemini batch failed: {e}")

# Save batch status
with open("batch_status.json", "w") as f:
    json.dump(batch_status, f, indent=2)
print(f"\nBatch status saved to batch_status.json")
#+end_src

#+RESULTS:
: Submitting Gemini batch...
:   Gemini batch ID: batches/rx8s0qs156icu8hca553bo8dwb9ro2pcgdgp
: 
: Batch status saved to batch_status.json

* Batch API: Check Status

Poll batch job status and download results when complete.

#+begin_src python
def check_openai_batch(batch_id: str) -> Tuple[str, Optional[List[dict]]]:
    """Check OpenAI batch status. Returns (status, results_or_none)."""
    batch = openai_client.batches.retrieve(batch_id)
    if batch.status == "completed":
        # Download results
        output_file = openai_client.files.content(batch.output_file_id)
        results = []
        for line in output_file.text.strip().split("\n"):
            result = json.loads(line)
            results.append({
                "id": result["custom_id"],
                "response": result["response"]["body"]["choices"][0]["message"]["content"],
                "provider": "openai"
            })
        return "completed", results
    return batch.status, None


def check_anthropic_batch(batch_id: str) -> Tuple[str, Optional[List[dict]]]:
    """Check Anthropic batch status. Returns (status, results_or_none)."""
    batch = anthropic_client.messages.batches.retrieve(batch_id)
    if batch.processing_status == "ended":
        results = []
        for result in anthropic_client.messages.batches.results(batch_id):
            results.append({
                "id": result.custom_id,
                "response": result.result.message.content[0].text,
                "provider": "anthropic"
            })
        return "completed", results
    return batch.processing_status, None


def check_gemini_batch(batch_name: str) -> Tuple[str, Optional[List[dict]]]:
    """Check Gemini batch status using SDK. Returns (status, results_or_none)."""
    if not google_client:
        return "no_client", None

    batch = google_client.batches.get(name=batch_name)

    if batch.state.name != "JOB_STATE_SUCCEEDED":
        return batch.state.name.lower(), None

    # Batch complete - extract results
    results = []
    for i, resp in enumerate(batch.responses):
        # responses are in order of input requests
        text = resp.candidates[0].content.parts[0].text if resp.candidates else "ERROR: No candidates"
        results.append({
            "id": f"lof_{i+1:03d}",  # Match original case IDs
            "response": text,
            "provider": "google"
        })

    return "completed", results


# Load batch status
with open("batch_status.json", "r") as f:
    batch_status = json.load(f)

print("Checking batch status...")
all_complete = True

for provider, info in batch_status["batches"].items():
    if provider == "openai" and openai_client:
        status, results = check_openai_batch(info["id"])
        batch_status["batches"]["openai"]["status"] = status
        if results:
            batch_status["batches"]["openai"]["results"] = results
        print(f"  OpenAI: {status}")

    elif provider == "anthropic" and anthropic_client:
        status, results = check_anthropic_batch(info["id"])
        batch_status["batches"]["anthropic"]["status"] = status
        if results:
            batch_status["batches"]["anthropic"]["results"] = results
        print(f"  Anthropic: {status}")

    elif provider == "google":
        status, results = check_gemini_batch(info["id"])
        batch_status["batches"]["google"]["status"] = status
        if results:
            batch_status["batches"]["google"]["results"] = results
        print(f"  Google: {status}")

    if batch_status["batches"][provider]["status"] != "completed":
        all_complete = False

# Update batch status file
with open("batch_status.json", "w") as f:
    json.dump(batch_status, f, indent=2)

if all_complete:
    print("\nAll batches complete! Run the next cell to process results.")
else:
    print("\nSome batches still processing. Re-run this cell to check again.")
#+end_src

#+RESULTS:
: Checking batch status...
:   Google: processing
: 
: Some batches still processing. Re-run this cell to check again.

* Process Batch Results

Combine batch results into unified format.

#+begin_src python
# Load batch status with results
with open("batch_status.json", "r") as f:
    batch_status = json.load(f)

# Build case lookup
case_lookup = {c["id"]: c for c in test_cases}

# Process all results
all_results = []
for provider, info in batch_status["batches"].items():
    if "results" not in info:
        print(f"Warning: No results for {provider}")
        continue

    for r in info["results"]:
        case = case_lookup[r["id"]]
        answer = extract_answer(r["response"])
        all_results.append({
            "id": r["id"],
            "input": case["input"],
            "target": case["target"],
            "difficulty": case["difficulty"],
            "depth": case.get("depth", 0),
            "provider": provider,
            "model": info["model"],
            "response": r["response"],
            "extracted_answer": answer,
            "correct": answer == case["target"]
        })

# Save full results
output = {
    "metadata": {
        "timestamp": datetime.now().isoformat(),
        "n_cases": len(test_cases),
        "prompt_template": PROMPT_TEMPLATE
    },
    "results": all_results
}

with open("results.json", "w") as f:
    json.dump(output, f, indent=2)

print(f"Saved {len(all_results)} results to results.json")
#+end_src

#+RESULTS:
: Saved 200 results to results.json

* Full Evaluation Run (Sequential Fallback) :ARCHIVE:

Use this if batch APIs are unavailable.

#+begin_src python :eval no
# Full run - this will take a while and cost API credits
print("Running full evaluation...")
all_results = run_eval(test_cases)

# Save results
output = {
    "metadata": {
        "timestamp": datetime.now().isoformat(),
        "n_cases": len(test_cases),
        "prompt_template": PROMPT_TEMPLATE
    },
    "results": all_results
}

with open("results.json", "w") as f:
    json.dump(output, f, indent=2)

print(f"Saved {len(all_results)} results to results.json")
#+end_src

* Analysis

#+begin_src python
def analyze_results(results: List[dict]) -> dict:
    """Compute accuracy metrics from results."""
    from collections import defaultdict

    # Overall accuracy by model
    by_model = defaultdict(lambda: {"correct": 0, "total": 0})
    for r in results:
        by_model[r["model"]]["total"] += 1
        if r["correct"]:
            by_model[r["model"]]["correct"] += 1

    # Accuracy by difficulty
    by_diff = defaultdict(lambda: defaultdict(lambda: {"correct": 0, "total": 0}))
    for r in results:
        by_diff[r["model"]][r["difficulty"]]["total"] += 1
        if r["correct"]:
            by_diff[r["model"]][r["difficulty"]]["correct"] += 1

    # Failure examples
    failures = [r for r in results if not r["correct"]]

    return {
        "by_model": {m: {"accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
                        "correct": d["correct"], "total": d["total"]}
                     for m, d in by_model.items()},
        "by_difficulty": {m: {diff: {"accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
                                     "correct": d["correct"], "total": d["total"]}
                              for diff, d in diffs.items()}
                          for m, diffs in by_diff.items()},
        "failures": failures[:10]  # First 10 failures
    }


# Analyze sample results (or load from file for full results)
if 'sample_results' in dir() and sample_results:
    analysis = analyze_results(sample_results)

    print("=== Accuracy by Model ===")
    for model, stats in analysis["by_model"].items():
        pct = stats["accuracy"] * 100
        print(f"{model}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

    print("\n=== Accuracy by Difficulty ===")
    for model, diffs in analysis["by_difficulty"].items():
        print(f"\n{model}:")
        for diff, stats in sorted(diffs.items()):
            pct = stats["accuracy"] * 100
            print(f"  {diff}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

    if analysis["failures"]:
        print("\n=== Sample Failures ===")
        for f in analysis["failures"][:3]:
            print(f"\n{f['model']}: {f['input']}")
            print(f"  Expected: {f['target']}, Got: {f['extracted_answer']}")
            print(f"  Response excerpt: {f['response'][:200]}...")
#+end_src

#+RESULTS:
#+begin_example
=== Accuracy by Model ===
gpt-5.1: 80.0% (4/5)
claude-sonnet-4-5-20250929: 40.0% (2/5)
gemini-2.5-pro: 20.0% (1/5)

=== Accuracy by Difficulty ===

gpt-5.1:
  easy: 100.0% (1/1)
  hard: 100.0% (1/1)
  medium: 66.7% (2/3)

claude-sonnet-4-5-20250929:
  easy: 0.0% (0/1)
  hard: 100.0% (1/1)
  medium: 33.3% (1/3)

gemini-2.5-pro:
  easy: 0.0% (0/1)
  hard: 0.0% (0/1)
  medium: 33.3% (1/3)

=== Sample Failures ===

claude-sonnet-4-5-20250929: (()())((())(()))
  Expected: (), Got: void
  Response excerpt: I'll simplify this expression step by step using the two axioms.

Starting expression: (()())((())(()))

Let me work on each part systematically.

,**Step 1:** Look at the first part: (()())
- Inside w...

gemini-2.5-pro: (()())((())(()))
  Expected: (), Got: void
  Response excerpt: Let's simplify the expression `(()())((())(()))` step-by-step using the given axioms:

,**Axioms:**
I1 (Number): `()()` = `()`
I2 (Order): `(())` = `void`

,**Expression:** `(()())((())(()))`

,**Step 1:...

claude-sonnet-4-5-20250929: ((()))(((())(()))(()))
  Expected: (), Got: void
  Response excerpt: I'll simplify this expression step by step using the two axioms.

,**Starting expression:** ((()))(((())(()))(()))

Let me work from the innermost parentheses outward.

,**Step 1:** Identify innermost e...
#+end_example

* Load and Analyze Saved Results

#+begin_src python
# Load previously saved results
with open("results.json", "r") as f:
    saved = json.load(f)

print(f"Loaded {len(saved['results'])} results from {saved['metadata']['timestamp']}")
analysis = analyze_results(saved['results'])
    
# Print full analysis
print("\n=== FULL RESULTS ===\n")
print("=== Accuracy by Model ===")
for model, stats in analysis["by_model"].items():
    pct = stats["accuracy"] * 100
    print(f"{model}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

print("\n=== Accuracy by Difficulty ===")
for model, diffs in analysis["by_difficulty"].items():
    print(f"\n{model}:")
    for diff, stats in sorted(diffs.items()):
        pct = stats["accuracy"] * 100
        print(f"  {diff}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

if analysis["failures"]:
    print("\n=== Sample Failures ===")
    for f in analysis["failures"][:3]:
        print(f"\n{f['model']}: {f['input']}")
        print(f"  Expected: {f['target']}, Got: {f['extracted_answer']}")
        print(f"  Response excerpt: {f['response'][:200]}...")
#+end_src

#+RESULTS:
#+begin_example
Loaded 200 results from 2025-12-05T19:15:32.000533

=== FULL RESULTS ===

=== Accuracy by Model ===
gpt-5.1: 81.0% (81/100)
claude-sonnet-4-5-20250929: 77.0% (77/100)

=== Accuracy by Difficulty ===

gpt-5.1:
  easy: 96.7% (29/30)
  hard: 70.0% (21/30)
  medium: 77.5% (31/40)

claude-sonnet-4-5-20250929:
  easy: 86.7% (26/30)
  hard: 66.7% (20/30)
  medium: 77.5% (31/40)

=== Sample Failures ===

gpt-5.1: ((()()))
  Expected: (), Got: void
  Response excerpt: Start with the expression:

1. `((()()))`

Work from the inside out.

2. Identify the innermost subexpression: `()`
   - This is already a single mark; no change.

3. Next subexpression: `(())` appear...

gpt-5.1: (((())))
  Expected: void, Got: ()
  Response excerpt: Start with the expression:

1. `(((())))`

Count the parentheses to see the structure:

- Outermost pair: `(          )`
  - Inside: `(((())))`
    - Inside that: `((()))`
      - Inside that: `(())`
...

gpt-5.1: ((((())(())())((()))))(((()(()))((())(())))(((())(()))((())(())(()))())((()(()))()(()(()))))
  Expected: (), Got: void
  Response excerpt: Let’s simplify step by step, using only:

- I1 (Number): `()() = ()`
- I2 (Order): `(()) =` void

---

### 1. Rewrite the expression clearly

Original:

```
((((())(())())((()))))(((()(()))((())(())))...
#+end_example

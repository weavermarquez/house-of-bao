#+title: Laws of Form Arithmetic Evaluation Suite
#+author: Valerie Kim
#+property: header-args:python :session lof :results output :python ".venv/bin/python"

* Introduction

This notebook evaluates whether Large Language Models can correctly simplify
Laws of Form (LoF) arithmetic expressions using Spencer-Brown's two axioms.

** The Calculus of Indications

George Spencer-Brown's /Laws of Form/ (1969) introduces a minimal calculus
based on a single symbol: the /mark/ or /cross/, written as =()=.

The calculus has only two axioms:

| Axiom | Name    | Rule           | Interpretation      |
|-------+---------+----------------+---------------------|
| I1    | Number  | =()()= → =()=  | Condense/Confirm    |
| I2    | Order   | =(())= → void  | Cancel/Compensate   |

- *I1 (Number)*: Multiple adjacent marks condense to a single mark
- *I2 (Order)*: A mark containing only a mark cancels to void (nothing)

Every well-formed expression reduces to exactly one of two values:
- =()= (the mark) — equivalent to TRUE or 1
- void (empty) — equivalent to FALSE or 0

* Setup

#+begin_src python
import os
import json
import random
import re
from datetime import datetime
from typing import Tuple, List, Optional

# API clients (we'll initialize these when needed)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY")

print("Setup complete.")
print(f"OpenAI key: {'set' if OPENAI_API_KEY else 'missing'}")
print(f"Anthropic key: {'set' if ANTHROPIC_API_KEY else 'missing'}")
print(f"Google key: {'set' if GOOGLE_API_KEY else 'missing'}")
#+end_src

#+RESULTS:
: Setup complete.
: OpenAI key: set
: Anthropic key: set
: Google key: missing

* Form Representation

We use two representations:
- *Internal*: Nested Python lists. =[[]]= represents =(())=, =[[], []]= represents =()()=
- *String*: Parentheses notation for display and LLM prompts

#+begin_src python
def form_to_string(form: list) -> str:
    """Convert internal form representation to string."""
    if not form:
        return ""
    return "".join(f"({form_to_string(child)})" for child in form)

def string_to_form(s: str) -> list:
    """Parse string notation to internal form."""
    result = []
    i = 0
    while i < len(s):
        if s[i] == '(':
            # Find matching close paren
            depth = 1
            j = i + 1
            while j < len(s) and depth > 0:
                if s[j] == '(':
                    depth += 1
                elif s[j] == ')':
                    depth -= 1
                j += 1
            # Recursively parse contents
            result.append(string_to_form(s[i+1:j-1]))
            i = j
        else:
            i += 1
    return result

# Test
test_forms = ["()", "(())", "()()", "((()))", "(()())"]
for s in test_forms:
    f = string_to_form(s)
    back = form_to_string(f)
    print(f"{s:12} -> {str(f):20} -> {back}")
#+end_src

#+RESULTS:
: ()           -> [[]]                 -> ()
: (())         -> [[[]]]               -> (())
: ()()         -> [[], []]             -> ()()
: ((()))       -> [[[[]]]]             -> ((()))
: (()())       -> [[[], []]]           -> (()())

* Form Generator

Generate random well-formed LoF expressions with controlled complexity.

#+begin_src python
def form_depth(form: list) -> int:
    """Calculate the nesting depth of a form."""
    if not form:
        return 0
    return 1 + max(form_depth(child) for child in form)


def generate_form(min_depth: int = 1, max_depth: int = 3, max_width: int = 3, depth: int = 0) -> list:
    """
    Generate a random LoF form with guaranteed minimum depth.

    Args:
        min_depth: Minimum nesting depth (guaranteed)
        max_depth: Maximum nesting depth
        max_width: Maximum number of adjacent forms at any level
        depth: Current depth (internal use)

    Returns:
        A form as nested lists
    """
    remaining_min = min_depth - depth
    remaining_max = max_depth - depth

    if remaining_max <= 0:
        # At max depth, return empty (void) or single mark
        return [[]] if random.random() > 0.5 else []

    if remaining_min > 0:
        # Must continue building to meet min_depth
        width = random.randint(1, max_width)  # At least 1
    else:
        # Can optionally terminate
        width = random.randint(0, max_width)
        if width == 0:
            return []

    result = []
    # Ensure at least one child reaches min_depth
    guaranteed_deep = random.randint(0, width - 1) if remaining_min > 0 else -1

    for i in range(width):
        if i == guaranteed_deep:
            # This child must reach min_depth
            child_form = generate_form(min_depth, max_depth, max_width, depth + 1)
        else:
            # This child can be shallower
            child_form = generate_form(0, max_depth, max_width, depth + 1)
        result.append(child_form)

    return result


# Generate some examples
random.seed(1010101423)
print("Sample generated forms (min_depth=2, max_depth=3):")
for i in range(10):
    form = generate_form(min_depth=2, max_depth=3, max_width=2)
    s = form_to_string(form)
    d = form_depth(form)
    print(f"  {i+1:2}. depth={d}: {s if s else '<void>'}")
#+end_src

#+RESULTS:
#+begin_example
Sample generated forms (min_depth=2, max_depth=3):
   1. depth=4: ((())((())))
   2. depth=4: (((()))(()(())))
   3. depth=4: ((())((())))
   4. depth=4: (((())))((()))
   5. depth=2: (()())
   6. depth=4: (((())()))
   7. depth=4: (((())))
   8. depth=3: ((()))((()())(()))
   9. depth=4: (((())())(()))
  10. depth=4: ((()(()))((())()))(((())(())))
#+end_example

* Ground-Truth Simplifier

Apply I1 and I2 exhaustively until fixed point, tracking each step.

#+begin_src python
def simplify_step(form: list) -> Tuple[list, Optional[str]]:
    """
    Apply one simplification step if possible.

    Returns:
        (new_form, axiom_applied) where axiom_applied is None if no change
    """
    # I1 (Number): IDENTICAL adjacent forms condense (aa = a)
    # Only applies when ALL adjacent forms are equal
    if len(form) > 1 and all(child == form[0] for child in form):
        return [form[0]], "I1"

    # I2 (Order): (()) = void
    # A mark is (()) if its content is [[]] (representing "()")
    # Check each top-level mark and remove any that are (())
    for i, child in enumerate(form):
        if child == [[]]:
            # This mark is (()) = void, remove it
            new_form = form[:i] + form[i+1:]
            return new_form, "I2"

    # Recursively check children
    for i, child in enumerate(form):
        new_child, axiom = simplify_step(child)
        if axiom:
            new_form = form[:i] + [new_child] + form[i+1:]
            return new_form, axiom

    return form, None


def simplify(form: list) -> Tuple[list, List[Tuple[str, str]]]:
    """
    Fully simplify a form to canonical form.

    Returns:
        (canonical_form, steps) where steps is list of (form_string, axiom)
    """
    steps = []
    current = form

    while True:
        new_form, axiom = simplify_step(current)
        if axiom is None:
            break
        steps.append((form_to_string(current), axiom))
        current = new_form

    return current, steps


def canonical_string(form: list) -> str:
    """Get the canonical string representation."""
    simplified, _ = simplify(form)
    s = form_to_string(simplified)
    return s if s else "void"


# Test the simplifier
print("Simplification examples:")
test_exprs = ["()()", "(())", "((()))", "(()())", "()(())", "(()(()))", "(())()", "(())(())"]
for expr in test_exprs:
    form = string_to_form(expr)
    result, steps = simplify(form)
    result_str = form_to_string(result) if result else "void"
    print(f"\n{expr} -> {result_str}")
    for step_form, axiom in steps:
        print(f"  {step_form} [{axiom}]")
#+end_src

#+RESULTS:
#+begin_example
Simplification examples:

()() -> ()
  ()() [I1]

(()) -> void
  (()) [I2]

((())) -> ()
  ((())) [I2]

(()()) -> void
  (()()) [I1]
  (()) [I2]

()(()) -> ()
  ()(()) [I2]

(()(())) -> void
  (()(())) [I2]
  (()) [I2]

(())() -> ()
  (())() [I2]

(())(()) -> void
  (())(()) [I1]
  (()) [I2]
#+end_example

* Test Case Generation

Generate a reproducible test suite across difficulty levels.

Difficulty is defined by *minimum* depth:
- Easy: depth 1-2 (simple marks and single nesting)
- Medium: depth 2-3 (requires at least one I2 application)
- Hard: depth 3-4 (multiple nested reductions)

#+begin_src python
def generate_test_cases(n: int = 100, seed: int = 2024) -> List[dict]:
    """Generate n test cases with varying difficulty."""
    random.seed(seed)
    cases = []

    # Distribution with (difficulty, min_depth, max_depth, max_width)
    # min_depth guarantees complexity for each difficulty level
    difficulties = (
        [("easy", 1, 2, 2)] * 30 +
        [("medium", 2, 3, 2)] * 40 +
        [("hard", 3, 4, 3)] * 30
    )
    random.shuffle(difficulties)

    for i, (diff, min_d, max_d, max_w) in enumerate(difficulties[:n]):
        form = generate_form(min_depth=min_d, max_depth=max_d, max_width=max_w)
        input_str = form_to_string(form) if form else "void"
        depth = form_depth(form)

        target = canonical_string(form)
        _, steps = simplify(form)

        cases.append({
            "id": f"lof_{i+1:03d}",
            "input": input_str,
            "target": target,
            "difficulty": diff,
            "depth": depth,
            "steps": len(steps)
        })

    return cases


# Generate and preview test cases
test_cases = generate_test_cases(100)
print(f"Generated {len(test_cases)} test cases\n")

# Preview distribution
from collections import Counter
diff_counts = Counter(c["difficulty"] for c in test_cases)
target_counts = Counter(c["target"] for c in test_cases)
depth_by_diff = {}
for c in test_cases:
    depth_by_diff.setdefault(c["difficulty"], []).append(c["depth"])

print("By difficulty:")
for diff in ["easy", "medium", "hard"]:
    depths = depth_by_diff.get(diff, [])
    avg_depth = sum(depths) / len(depths) if depths else 0
    print(f"  {diff}: {diff_counts[diff]} cases, avg depth={avg_depth:.1f}, range=[{min(depths)}-{max(depths)}]")

print("\nBy target value:")
for target, count in sorted(target_counts.items()):
    print(f"  {target}: {count}")

print("\nSample cases:")
for case in test_cases[:5]:
    print(f"  {case['id']}: {case['input']:20} -> {case['target']:6} (d={case['depth']}, {case['difficulty']}, {case['steps']} steps)")
#+end_src

#+RESULTS:
#+begin_example
Generated 100 test cases

By difficulty:
  easy: 30 cases, avg depth=2.3, range=[1-3]
  medium: 40 cases, avg depth=3.5, range=[2-4]
  hard: 30 cases, avg depth=4.8, range=[3-5]

By target value:
  (): 63
  void: 37

Sample cases:
  lof_001: (()())((())(()))     -> ()     (d=3, easy, 4 steps)
  lof_002: ((()))(((())(()))(())) -> ()     (d=4, medium, 5 steps)
  lof_003: ((()((())(())(())))) -> ()     (d=5, hard, 4 steps)
  lof_004: (())                 -> void   (d=2, medium, 1 steps)
  lof_005: (((())()))           -> ()     (d=4, medium, 2 steps)
#+end_example

* Save Test Cases

#+begin_src python
# Save test cases for reproducibility
with open("test_cases.json", "w") as f:
    json.dump(test_cases, f, indent=2)
print(f"Saved {len(test_cases)} test cases to test_cases.json")
#+end_src

#+RESULTS:
: Saved 100 test cases to test_cases.json

* LLM Clients

Initialize API clients for each provider.

#+begin_src python
# OpenAI client
openai_client = None
if OPENAI_API_KEY:
    import openai
    openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)
    print("OpenAI client initialized")

# Anthropic client
anthropic_client = None
if ANTHROPIC_API_KEY:
    import anthropic
    anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)
    print("Anthropic client initialized")

# Google client
google_model = None
if GOOGLE_API_KEY:
    import google.generativeai as genai
    genai.configure(api_key=GOOGLE_API_KEY)
    google_model = genai.GenerativeModel("gemini-2.0-flash")
    print("Google Gemini client initialized")
    
#+end_src

#+RESULTS:

* Eval Functions

#+begin_src python
PROMPT_TEMPLATE = """Simplify this Laws of Form expression using the two axioms:

I1 (Number): ()() = ()
   Multiple adjacent marks condense to a single mark.

I2 (Order): (()) = void
   A mark containing only a mark cancels to void (nothing).

Expression: {expression}

Apply the axioms step by step until you reach either:
- () (the mark)
- void (empty/nothing)

Show your reasoning, then state your final answer on the last line as either:
FINAL: ()
or
FINAL: void
"""


def extract_answer(response: str) -> str:
    """Extract the final answer from LLM response."""
    # Look for FINAL: pattern
    match = re.search(r'FINAL:\s*(\(\)|void)', response, re.IGNORECASE)
    if match:
        ans = match.group(1).lower()
        return "()" if ans == "()" else "void"

    # Fallback: look for last occurrence of () or void
    response_lower = response.lower()
    last_mark = response_lower.rfind("()")
    last_void = max(response_lower.rfind("void"), response_lower.rfind("empty"))

    if last_mark > last_void:
        return "()"
    elif last_void > last_mark:
        return "void"

    return "unknown"


def eval_openai(expression: str, model: str = "gpt-5.1") -> Tuple[str, str]:
    """Evaluate using OpenAI API."""
    if not openai_client:
        return "", "no_client"

    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = openai_client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0
    )
    text = response.choices[0].message.content
    return text, extract_answer(text)

def eval_anthropic(expression: str, model: str = "claude-sonnet-4-5-20250929") -> Tuple[str, str]:
    """Evaluate using Anthropic API."""
    if not anthropic_client:
        return "", "no_client"

    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = anthropic_client.messages.create(
        model=model,
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    )
    text = response.content[0].text
    return text, extract_answer(text)


def eval_google(expression: str) -> Tuple[str, str]:
    """Evaluate using Google Gemini API."""
    if not google_model:
        return "", "no_client"

    prompt = PROMPT_TEMPLATE.format(expression=expression)
    response = google_model.generate_content(prompt)
    text = response.text
    return text, extract_answer(text)


print("Eval functions defined.")
#+end_src

#+RESULTS:
: Eval functions defined.

* Run Evaluation

#+begin_src python
def run_eval(test_cases: List[dict], models: List[str] = None) -> List[dict]:
    """Run evaluation across all test cases and models."""
    if models is None:
        models = []
        if openai_client:
            models.append(("openai", "gpt-5.1", eval_openai))
        if anthropic_client:
            models.append(("anthropic", "claude-sonnet-4-5-20250929", eval_anthropic))
        if google_model:
            models.append(("google", "gemini-2.5-flash", eval_google))

    results = []
    total = len(test_cases) * len(models)
    done = 0

    for case in test_cases:
        for provider, model_name, eval_fn in models:
            try:
                response, answer = eval_fn(case["input"])
                correct = answer == case["target"]
            except Exception as e:
                response = f"ERROR: {str(e)}"
                answer = "error"
                correct = False

            results.append({
                "id": case["id"],
                "input": case["input"],
                "target": case["target"],
                "difficulty": case["difficulty"],
                "provider": provider,
                "model": model_name,
                "response": response,
                "extracted_answer": answer,
                "correct": correct
            })

            done += 1
            if done % 10 == 0:
                print(f"Progress: {done}/{total}")

    return results


# Run on a small subset first to test
print("Testing with 5 cases...")
sample_results = run_eval(test_cases[:5])
for r in sample_results:
    status = "✓" if r["correct"] else "✗"
    print(f"{status} {r['id']} ({r['provider']}): {r['input']} -> {r['extracted_answer']} (target: {r['target']})")
#+end_src

#+RESULTS:
#+begin_example
Testing with 5 cases...
Progress: 10/10
✗ lof_001 (openai): (()())((())(())) -> void (target: ())
✗ lof_001 (anthropic): (()())((())(())) -> void (target: ())
✓ lof_002 (openai): ((()))(((())(()))(())) -> () (target: ())
✓ lof_002 (anthropic): ((()))(((())(()))(())) -> () (target: ())
✓ lof_003 (openai): ((()((())(())(())))) -> () (target: ())
✗ lof_003 (anthropic): ((()((())(())(())))) -> void (target: ())
✓ lof_004 (openai): (()) -> void (target: void)
✓ lof_004 (anthropic): (()) -> void (target: void)
✗ lof_005 (openai): (((())())) -> void (target: ())
✗ lof_005 (anthropic): (((())())) -> void (target: ())
#+end_example




With 4o and Sonnet-4:

#+RESULTS:
#+begin_example
Testing with 5 cases...
Progress: 10/10
✓ lof_001 (openai): (()())((())(())) -> () (target: ())
✗ lof_001 (anthropic): (()())((())(())) -> void (target: ())
✓ lof_002 (openai): ((()))(((())(()))(())) -> () (target: ())
✓ lof_002 (anthropic): ((()))(((())(()))(())) -> () (target: ())
✓ lof_003 (openai): ((()((())(())(())))) -> () (target: ())
✗ lof_003 (anthropic): ((()((())(())(())))) -> void (target: ())
✓ lof_004 (openai): (()) -> void (target: void)
✓ lof_004 (anthropic): (()) -> void (target: void)
✗ lof_005 (openai): (((())())) -> void (target: ())
✗ lof_005 (anthropic): (((())())) -> void (target: ())
#+end_example

* Full Evaluation Run

Uncomment and run this to evaluate all test cases.

#+begin_src python :eval no
# Full run - this will take a while and cost API credits
print("Running full evaluation...")
all_results = run_eval(test_cases)

# Save results
output = {
    "metadata": {
        "timestamp": datetime.now().isoformat(),
        "n_cases": len(test_cases),
        "prompt_template": PROMPT_TEMPLATE
    },
    "results": all_results
}

with open("results.json", "w") as f:
    json.dump(output, f, indent=2)

print(f"Saved {len(all_results)} results to results.json")
#+end_src

* Analysis

#+begin_src python
def analyze_results(results: List[dict]) -> dict:
    """Compute accuracy metrics from results."""
    from collections import defaultdict

    # Overall accuracy by model
    by_model = defaultdict(lambda: {"correct": 0, "total": 0})
    for r in results:
        by_model[r["model"]]["total"] += 1
        if r["correct"]:
            by_model[r["model"]]["correct"] += 1

    # Accuracy by difficulty
    by_diff = defaultdict(lambda: defaultdict(lambda: {"correct": 0, "total": 0}))
    for r in results:
        by_diff[r["model"]][r["difficulty"]]["total"] += 1
        if r["correct"]:
            by_diff[r["model"]][r["difficulty"]]["correct"] += 1

    # Failure examples
    failures = [r for r in results if not r["correct"]]

    return {
        "by_model": {m: {"accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
                        "correct": d["correct"], "total": d["total"]}
                     for m, d in by_model.items()},
        "by_difficulty": {m: {diff: {"accuracy": d["correct"]/d["total"] if d["total"] > 0 else 0,
                                     "correct": d["correct"], "total": d["total"]}
                              for diff, d in diffs.items()}
                          for m, diffs in by_diff.items()},
        "failures": failures[:10]  # First 10 failures
    }


# Analyze sample results (or load from file for full results)
if 'sample_results' in dir() and sample_results:
    analysis = analyze_results(sample_results)

    print("=== Accuracy by Model ===")
    for model, stats in analysis["by_model"].items():
        pct = stats["accuracy"] * 100
        print(f"{model}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

    print("\n=== Accuracy by Difficulty ===")
    for model, diffs in analysis["by_difficulty"].items():
        print(f"\n{model}:")
        for diff, stats in sorted(diffs.items()):
            pct = stats["accuracy"] * 100
            print(f"  {diff}: {pct:.1f}% ({stats['correct']}/{stats['total']})")

    if analysis["failures"]:
        print("\n=== Sample Failures ===")
        for f in analysis["failures"][:3]:
            print(f"\n{f['model']}: {f['input']}")
            print(f"  Expected: {f['target']}, Got: {f['extracted_answer']}")
            print(f"  Response excerpt: {f['response'][:200]}...")
#+end_src

* Load and Analyze Saved Results

#+begin_src python :eval no
# Load previously saved results
with open("results.json", "r") as f:
    saved = json.load(f)

print(f"Loaded {len(saved['results'])} results from {saved['metadata']['timestamp']}")
analysis = analyze_results(saved['results'])

# Print full analysis
print("\n=== FULL RESULTS ===\n")
for model, stats in analysis["by_model"].items():
    pct = stats["accuracy"] * 100
    print(f"{model}: {pct:.1f}% ({stats['correct']}/{stats['total']})")
#+end_src
